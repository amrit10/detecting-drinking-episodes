{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import scipy\n",
    "from scipy.stats import skew, kurtosis\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JB3156</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CC6740</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SA0297</td>\n",
       "      <td>0.0758</td>\n",
       "      <td>0.0273</td>\n",
       "      <td>-0.0102</td>\n",
       "      <td>1493733882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SA0297</td>\n",
       "      <td>-0.0359</td>\n",
       "      <td>0.0794</td>\n",
       "      <td>0.0037</td>\n",
       "      <td>1493733882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SA0297</td>\n",
       "      <td>-0.2427</td>\n",
       "      <td>-0.0861</td>\n",
       "      <td>-0.0163</td>\n",
       "      <td>1493733882</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      pid       x       y       z        time\n",
       "0  JB3156  0.0000  0.0000  0.0000           0\n",
       "1  CC6740  0.0000  0.0000  0.0000           0\n",
       "2  SA0297  0.0758  0.0273 -0.0102  1493733882\n",
       "3  SA0297 -0.0359  0.0794  0.0037  1493733882\n",
       "4  SA0297 -0.2427 -0.0861 -0.0163  1493733882"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read Accelerometer Data\n",
    "acc_data = pd.read_csv('../data/all_accelerometer_data_pids_13.csv')\n",
    "\n",
    "\n",
    "def get_time_value(x):\n",
    "  # x is ms. it is divided by 1000 to get microsecond\n",
    "  t = datetime.datetime.fromtimestamp(x/1000.0)\n",
    "  t = t.replace(microsecond = 0)\n",
    "  return int(t.timestamp())\n",
    "\n",
    "acc_data['window10'] = acc_data['time'].apply(get_time_value)\n",
    "acc_data = acc_data.drop(columns=\"time\")\n",
    "acc_data = acc_data.rename(columns = {\"window10\": \"time\"})\n",
    "\n",
    "acc_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['JB3156', 'CC6740', 'SA0297', 'PC6771', 'BK7610', 'DC6359',\n",
       "       'MC7070', 'MJ8002', 'BU4707', 'JR8022', 'HV0618', 'SF3079',\n",
       "       'DK3500'], dtype=object)"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pids = acc_data['pid'].unique()\n",
    "pids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>TAC_Reading</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.700000e+01</td>\n",
       "      <td>57.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.493758e+09</td>\n",
       "      <td>0.228070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.841595e+04</td>\n",
       "      <td>0.423318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.493719e+09</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.493729e+09</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.493756e+09</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.493782e+09</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.493808e+09</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          timestamp  TAC_Reading\n",
       "count  5.700000e+01    57.000000\n",
       "mean   1.493758e+09     0.228070\n",
       "std    2.841595e+04     0.423318\n",
       "min    1.493719e+09     0.000000\n",
       "25%    1.493729e+09     0.000000\n",
       "50%    1.493756e+09     0.000000\n",
       "75%    1.493782e+09     0.000000\n",
       "max    1.493808e+09     1.000000"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read clean tac data for pid = BK7610\n",
    "clean_tac_data = pd.DataFrame()\n",
    "for pid in pids:\n",
    "    temp = pd.read_csv('../data/clean_tac/' + pid + '_clean_TAC.csv')\n",
    "    # clean_tac_data = pd.read_csv('../data/clean_tac/JB3156_clean_TAC.csv')\n",
    "    clean_tac_data.append(temp)\n",
    "    \n",
    "clean_tac_data[\"tac\"] = np.where(clean_tac_data[\"TAC_Reading\"] > 0.08, 1, 0)\n",
    "clean_tac_data = clean_tac_data.drop(columns=\"TAC_Reading\")\n",
    "clean_tac_data = clean_tac_data.rename(columns={\"tac\": \"TAC_Reading\"})\n",
    "clean_tac_data.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BK7610'], dtype=object)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtering for specific PID (temps)\n",
    "acc_data_pid = acc_data[acc_data.pid == \"BK7610\"]\n",
    "acc_data_pid['pid'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.225727e+06</td>\n",
       "      <td>1.225727e+06</td>\n",
       "      <td>1.225727e+06</td>\n",
       "      <td>1.225727e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-6.497703e-03</td>\n",
       "      <td>7.507374e-03</td>\n",
       "      <td>2.747567e-03</td>\n",
       "      <td>1.493752e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.380473e-01</td>\n",
       "      <td>1.387602e-01</td>\n",
       "      <td>1.279124e-01</td>\n",
       "      <td>9.276766e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-4.274800e+00</td>\n",
       "      <td>-6.948900e+00</td>\n",
       "      <td>-5.277200e+00</td>\n",
       "      <td>1.493736e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-9.400000e-03</td>\n",
       "      <td>-6.000000e-03</td>\n",
       "      <td>-7.300000e-03</td>\n",
       "      <td>1.493744e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>5.700000e-03</td>\n",
       "      <td>1.493752e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.300000e-03</td>\n",
       "      <td>9.400000e-03</td>\n",
       "      <td>1.140000e-02</td>\n",
       "      <td>1.493760e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.450300e+00</td>\n",
       "      <td>5.344100e+00</td>\n",
       "      <td>4.656500e+00</td>\n",
       "      <td>1.493768e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  x             y             z          time\n",
       "count  1.225727e+06  1.225727e+06  1.225727e+06  1.225727e+06\n",
       "mean  -6.497703e-03  7.507374e-03  2.747567e-03  1.493752e+09\n",
       "std    1.380473e-01  1.387602e-01  1.279124e-01  9.276766e+03\n",
       "min   -4.274800e+00 -6.948900e+00 -5.277200e+00  1.493736e+09\n",
       "25%   -9.400000e-03 -6.000000e-03 -7.300000e-03  1.493744e+09\n",
       "50%    1.000000e-04  1.000000e-04  5.700000e-03  1.493752e+09\n",
       "75%    8.300000e-03  9.400000e-03  1.140000e-02  1.493760e+09\n",
       "max    6.450300e+00  5.344100e+00  4.656500e+00  1.493768e+09"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_data_pid.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1225727, 5)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_data_pid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1493735870],\n",
       " [0, 1493735870],\n",
       " [0, 1493735870],\n",
       " [0, 1493735870],\n",
       " [0, 1493735870],\n",
       " [0, 1493735870],\n",
       " [0, 1493735870],\n",
       " [0, 1493735870],\n",
       " [0, 1493735870],\n",
       " [0, 1493735870],\n",
       " [0, 1493735870],\n",
       " [0, 1493735870],\n",
       " [0, 1493735870],\n",
       " [0, 1493735870],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " ...]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Up sampling tac data to match acc data\n",
    "clean_ts = clean_tac_data['timestamp'] \n",
    "acc_ts = acc_data_pid['time']\n",
    "all_labels = list()\n",
    "offset_tac, offset_acc = 0, 0\n",
    "# print(acc_ts.iloc[0])\n",
    "# print(clean_ts.loc[0])\n",
    "# print(clean_tac_data.loc[0]['TAC_Reading'])\n",
    "# # acc_ts.iloc[0] #1493735870653\n",
    "while offset_tac < len(clean_ts) and offset_acc < len(acc_ts):\n",
    "  \n",
    "  while acc_ts.iloc[offset_acc] < clean_ts.iloc[offset_tac]:\n",
    "    all_labels.append([clean_tac_data.iloc[offset_tac]['TAC_Reading'], acc_ts.iloc[offset_acc]])\n",
    "    offset_acc += 1\n",
    "    if offset_acc >= len(acc_ts):\n",
    "      break\n",
    "\n",
    "  offset_tac += 1\n",
    "\n",
    "all_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1225727, 2), (1225727, 5))"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_labels_df = pd.DataFrame(all_labels, columns = [\"tac\", \"time\"])\n",
    "all_labels_df.shape, acc_data_pid.shape\n",
    "\n",
    "# merged = merged.drop_duplicates().reset_index(drop=True)\n",
    "# merged.to_csv(\"../data/BK7610_final_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged = acc_data_pid.head(10).merge(all_labels_df.head(10), on = 'time', how='inner')\n",
    "# merged['time'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(all_labels_df['time'].unique()), len(acc_data_pid['time'].unique())\n",
    "clean_tac_data[\"timestamp\"].is_monotonic_increasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_data_pid['tac_reading'] = \n",
    "# TODO: Make sure tac data is sorted on timestamp\n",
    "clean_tac_data[\"from\"] = clean_tac_data[\"timestamp\"].shift(1, fill_value=-1) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tac_data.index = pd.IntervalIndex.from_arrays(clean_tac_data[\"from\"], clean_tac_data[\"timestamp\"], closed = \"both\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7y/kyw1v_8j0g1ckfb3g6x93q1m0000gn/T/ipykernel_92662/2638206493.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  acc_data_pid['tac'] = acc_data_pid[\"time\"].apply(lambda x: clean_tac_data.iloc[clean_tac_data.index.get_loc(x)][\"TAC_Reading\"])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>time</th>\n",
       "      <th>tac</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47136</th>\n",
       "      <td>BK7610</td>\n",
       "      <td>0.1261</td>\n",
       "      <td>-0.0078</td>\n",
       "      <td>-0.0243</td>\n",
       "      <td>1493735870</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47138</th>\n",
       "      <td>BK7610</td>\n",
       "      <td>0.1336</td>\n",
       "      <td>-0.0697</td>\n",
       "      <td>-0.0446</td>\n",
       "      <td>1493735870</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47140</th>\n",
       "      <td>BK7610</td>\n",
       "      <td>0.1443</td>\n",
       "      <td>-0.0474</td>\n",
       "      <td>-0.0447</td>\n",
       "      <td>1493735870</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47142</th>\n",
       "      <td>BK7610</td>\n",
       "      <td>0.1255</td>\n",
       "      <td>-0.0038</td>\n",
       "      <td>0.0111</td>\n",
       "      <td>1493735870</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47144</th>\n",
       "      <td>BK7610</td>\n",
       "      <td>0.1076</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0276</td>\n",
       "      <td>1493735870</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6071104</th>\n",
       "      <td>BK7610</td>\n",
       "      <td>-0.0784</td>\n",
       "      <td>-0.0161</td>\n",
       "      <td>0.1719</td>\n",
       "      <td>1493767770</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6071108</th>\n",
       "      <td>BK7610</td>\n",
       "      <td>-0.0395</td>\n",
       "      <td>-0.0816</td>\n",
       "      <td>0.1634</td>\n",
       "      <td>1493767770</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6071112</th>\n",
       "      <td>BK7610</td>\n",
       "      <td>0.0160</td>\n",
       "      <td>-0.0853</td>\n",
       "      <td>0.0906</td>\n",
       "      <td>1493767770</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6071117</th>\n",
       "      <td>BK7610</td>\n",
       "      <td>0.0901</td>\n",
       "      <td>-0.0767</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>1493767770</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6071121</th>\n",
       "      <td>BK7610</td>\n",
       "      <td>0.1700</td>\n",
       "      <td>-0.1390</td>\n",
       "      <td>-0.0773</td>\n",
       "      <td>1493767770</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1225727 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            pid       x       y       z        time  tac\n",
       "47136    BK7610  0.1261 -0.0078 -0.0243  1493735870    0\n",
       "47138    BK7610  0.1336 -0.0697 -0.0446  1493735870    0\n",
       "47140    BK7610  0.1443 -0.0474 -0.0447  1493735870    0\n",
       "47142    BK7610  0.1255 -0.0038  0.0111  1493735870    0\n",
       "47144    BK7610  0.1076  0.0032  0.0276  1493735870    0\n",
       "...         ...     ...     ...     ...         ...  ...\n",
       "6071104  BK7610 -0.0784 -0.0161  0.1719  1493767770    1\n",
       "6071108  BK7610 -0.0395 -0.0816  0.1634  1493767770    1\n",
       "6071112  BK7610  0.0160 -0.0853  0.0906  1493767770    1\n",
       "6071117  BK7610  0.0901 -0.0767  0.0162  1493767770    1\n",
       "6071121  BK7610  0.1700 -0.1390 -0.0773  1493767770    1\n",
       "\n",
       "[1225727 rows x 6 columns]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_data_pid['tac'] = acc_data_pid[\"time\"].apply(lambda x: clean_tac_data.iloc[clean_tac_data.index.get_loc(x)][\"TAC_Reading\"])\n",
    "acc_data_pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47136, 6071121)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(acc_data_pid[\"tac\"].unique().sort())\n",
    "# clean_tac_data\n",
    "# min: 1,493,718,714\n",
    "# max: 1,493,807,899\n",
    "# \n",
    "# acc_data_pid\n",
    "# min: 1,493,735,870\n",
    "# max: 1,493,767,770\n",
    "# acc_data_pid[\"time\"].max()\n",
    "# groups = acc_data_pid.groupby([\"time\"])\n",
    "# print(groups.apply(lambda x: x[\"tac\"]<20))\n",
    "# time_stamps = groups.apply(lambda x: x[\"tac\"]<20)\n",
    "# time_stamps \n",
    "\n",
    "min(acc_data_pid.index), max(acc_data_pid.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make n = 10 after either removing one record which has 7 records for a second or by adding 3 dummy values to it (latter is better)\n",
    "# frame_temp.groupby([ \"pid\", \"window10\"]).count().describe()\n",
    "# We are sampling with replacement, which should be okay since it is within a second\n",
    "acc_data_pid_20s = acc_data_pid.groupby([ \"pid\", \"time\"]).sample(n = 20, replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Make this as an assert statement in the begininng for both tac and accelerometer data\n",
    "acc_data_pid_20s[\"time\"].is_monotonic_increasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SLIDING WINDOW\n",
    "acc_data_sliding = acc_data_pid_20s.copy()\n",
    "# cols = [\"x\", \"y\", \"z\"]\n",
    "# window_size = 10 # including current\n",
    "\n",
    "# for col in cols:\n",
    "#     cols_to_append = []\n",
    "#     for i in range(0, window_size):\n",
    "#         shifted_col_name = str(col) + \"_\" + str(i)\n",
    "#         acc_data_sliding[shifted_col_name] = acc_data_sliding[col].shift(i, fill_value = 0)\n",
    "#         cols_to_append.append(shifted_col_name)\n",
    "    \n",
    "#     # we have (windpw_size ) columsn for col\n",
    "#     # Uncomment to keep original columns\n",
    "#     acc_data_sliding = acc_data_sliding.drop(columns=[col])    \n",
    "#     acc_data_sliding[str(col)] = acc_data_sliding[cols_to_append].values.tolist()\n",
    "    \n",
    "#     # acc_data_sliding = acc_data_sliding.drop(columns=cols_to_append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # x_sliding_window.shape\n",
    "pids = [\"BK7610\"]\n",
    "final = []\n",
    "labels = []\n",
    "for pid in pids:\n",
    "  # temptemp = acc_data_pid_20s[acc_data_pid_20s['pid'] == pid]\n",
    "  temptemp = acc_data_sliding[acc_data_sliding['pid'] == pid]\n",
    "  times = temptemp.time.unique()\n",
    "  final_temp =[]\n",
    "  labels_temp = []\n",
    "  for i in range(len(times)):\n",
    "    # x = np.lib.stride_tricks.sliding_window_view(frame_temp2[frame_temp2.pid == pid and frame_temp2.window10 == time], window_shape = 10)\n",
    "    # temptemptemp = temptemp[temptemp['time'] == time]\n",
    "    time_to_filter = [times[j] if j >= 0 else -1 for j in range(i-9, i+1)]\n",
    "    # print(time_to_filter)\n",
    "    # if i == 10:\n",
    "    #   break\n",
    "    temptemptemp = temptemp[temptemp['time'].isin(time_to_filter)]\n",
    "    # TODO: Create x y z sliding windows\n",
    "    # x = np.lib.stride_tricks.sliding_window_view(temptemptemp[\"x\"], window_shape = 10)\n",
    "    # y = np.lib.stride_tricks.sliding_window_view(temptemptemp[\"y\"], window_shape = 10)\n",
    "    # z = np.lib.stride_tricks.sliding_window_view(temptemptemp[\"z\"], window_shape = 10)\n",
    "\n",
    "    # x_dash = np.array(temptemptemp[[\"x_\"+str(i) for i in range(window_size)]]).flatten()\n",
    "    # y_dash = np.array(temptemptemp[[\"y_\"+str(i) for i in range(window_size)]]).flatten()\n",
    "    # z_dash = np.array(temptemptemp[[\"z_\"+str(i) for i in range(window_size)]]).flatten()\n",
    "\n",
    "    x_dash = np.array(temptemptemp[\"x\"])\n",
    "    y_dash = np.array(temptemptemp[\"y\"])\n",
    "    z_dash = np.array(temptemptemp[\"z\"])\n",
    "\n",
    "    x_dash = np.pad(x_dash, (200 - len(x_dash), 0), \"constant\")\n",
    "    y_dash = np.pad(y_dash, (200 - len(y_dash), 0), \"constant\")\n",
    "    z_dash = np.pad(z_dash, (200 - len(z_dash), 0), \"constant\")\n",
    "\n",
    "    # a = np.vstack((temptemptemp[\"x\"].apply(lambda x: np.array(x, dtype=\"float32\")), temptemptemp[\"y\"].apply(lambda x: np.array(x, dtype=\"float32\")), temptemptemp[\"z\"].apply(lambda x: np.array(x, dtype=\"float32\"))))\n",
    "    a = np.transpose(np.vstack((x_dash, y_dash, z_dash)))\n",
    "    final_temp.append(a)\n",
    "    labels_temp.append(temptemptemp.head(1)[\"tac\"])\n",
    "  final.append(np.array(final_temp))\n",
    "  labels.append(np.array(labels_temp))\n",
    "  # print(final)\n",
    "  \n",
    "  # break\n",
    "# print(np.array(final,dtype=object).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>time</th>\n",
       "      <th>tac</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47140</th>\n",
       "      <td>BK7610</td>\n",
       "      <td>0.1443</td>\n",
       "      <td>-0.0474</td>\n",
       "      <td>-0.0447</td>\n",
       "      <td>1493735870</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47152</th>\n",
       "      <td>BK7610</td>\n",
       "      <td>0.1133</td>\n",
       "      <td>0.0080</td>\n",
       "      <td>-0.0172</td>\n",
       "      <td>1493735870</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47140</th>\n",
       "      <td>BK7610</td>\n",
       "      <td>0.1443</td>\n",
       "      <td>-0.0474</td>\n",
       "      <td>-0.0447</td>\n",
       "      <td>1493735870</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47146</th>\n",
       "      <td>BK7610</td>\n",
       "      <td>0.1155</td>\n",
       "      <td>-0.0284</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1493735870</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47138</th>\n",
       "      <td>BK7610</td>\n",
       "      <td>0.1336</td>\n",
       "      <td>-0.0697</td>\n",
       "      <td>-0.0446</td>\n",
       "      <td>1493735870</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47856</th>\n",
       "      <td>BK7610</td>\n",
       "      <td>-0.0535</td>\n",
       "      <td>-0.0496</td>\n",
       "      <td>-0.0077</td>\n",
       "      <td>1493735879</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47804</th>\n",
       "      <td>BK7610</td>\n",
       "      <td>-0.0108</td>\n",
       "      <td>-0.0559</td>\n",
       "      <td>0.0268</td>\n",
       "      <td>1493735879</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47852</th>\n",
       "      <td>BK7610</td>\n",
       "      <td>-0.0672</td>\n",
       "      <td>-0.0969</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>1493735879</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47854</th>\n",
       "      <td>BK7610</td>\n",
       "      <td>-0.0636</td>\n",
       "      <td>-0.0894</td>\n",
       "      <td>0.0330</td>\n",
       "      <td>1493735879</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47810</th>\n",
       "      <td>BK7610</td>\n",
       "      <td>0.0384</td>\n",
       "      <td>0.0236</td>\n",
       "      <td>-0.1074</td>\n",
       "      <td>1493735879</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          pid       x       y       z        time  tac\n",
       "47140  BK7610  0.1443 -0.0474 -0.0447  1493735870    0\n",
       "47152  BK7610  0.1133  0.0080 -0.0172  1493735870    0\n",
       "47140  BK7610  0.1443 -0.0474 -0.0447  1493735870    0\n",
       "47146  BK7610  0.1155 -0.0284  0.0000  1493735870    0\n",
       "47138  BK7610  0.1336 -0.0697 -0.0446  1493735870    0\n",
       "...       ...     ...     ...     ...         ...  ...\n",
       "47856  BK7610 -0.0535 -0.0496 -0.0077  1493735879    0\n",
       "47804  BK7610 -0.0108 -0.0559  0.0268  1493735879    0\n",
       "47852  BK7610 -0.0672 -0.0969  0.0055  1493735879    0\n",
       "47854  BK7610 -0.0636 -0.0894  0.0330  1493735879    0\n",
       "47810  BK7610  0.0384  0.0236 -0.1074  1493735879    0\n",
       "\n",
       "[200 rows x 6 columns]"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# temptemptemp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 3)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [-0.1331,  0.026 ,  0.1165, -0.0039, -0.1326, -0.1095,  0.1165,\n",
    "        #  0.1467, -0.1494, -0.0233]\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 30735, 200, 3)\n",
      "(1, 30735, 1)\n"
     ]
    }
   ],
   "source": [
    "final_arr = np.asarray(final).astype('float32')\n",
    "\n",
    "# final_arr = np.reshape(final_arr, (final_arr.shape[0], final_arr.shape[1], final_arr.shape[3], final_arr.shape[2]))\n",
    "\n",
    "labels_arr = np.asarray(labels).astype('float32')\n",
    "print(final_arr.shape)\n",
    "print(labels_arr.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final_arr[0][9][0:19] == final_arr[0][0][180:199]\n",
    "# np.unique(labels_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30735, 200, 3), (30735, 1))"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_arr_reshape = np.reshape(final_arr, (30735, 200, 3)) \n",
    "labels_arr_reshape = np.reshape(labels_arr, (30735,1))\n",
    "final_arr_reshape.shape, labels_arr_reshape.shape \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = range(len(final_arr_reshape))\n",
    "indices = tf.random.shuffle(indices)\n",
    "\n",
    "final_arr_reshape = tf.gather(final_arr_reshape, indices)\n",
    "labels_arr_reshape = tf.gather(labels_arr_reshape, indices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from TF_Model import Drunk\n",
    "# drunk = Drunk()\n",
    "\n",
    "# drunk.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), \\\n",
    "#                    optimizer=tf.keras.optimizers.Adam(0.03), \\\n",
    "#                    metrics=[tf.keras.metrics.Accuracy()]) \n",
    "\n",
    "# drunk.build((30735, 600))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model \n",
    "#### 3 hidden layers (32 nodes, RELU, 32 nodes, RELU, 16 nodes, RELU)  \n",
    "#### output layer uses sigmoid activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_78\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_76 (Flatten)        (30735, 600)              0         \n",
      "                                                                 \n",
      " dense_250 (Dense)           (30735, 1024)             615424    \n",
      "                                                                 \n",
      " dense_251 (Dense)           (30735, 768)              787200    \n",
      "                                                                 \n",
      " dense_252 (Dense)           (30735, 512)              393728    \n",
      "                                                                 \n",
      " dense_253 (Dense)           (30735, 256)              131328    \n",
      "                                                                 \n",
      " dense_254 (Dense)           (30735, 128)              32896     \n",
      "                                                                 \n",
      " dense_255 (Dense)           (30735, 64)               8256      \n",
      "                                                                 \n",
      " dense_256 (Dense)           (30735, 32)               2080      \n",
      "                                                                 \n",
      " dense_257 (Dense)           (30735, 1)                33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,970,945\n",
      "Trainable params: 1,970,945\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model_data = np.reshape(final_arr, (30735, 200, 3)) \n",
    "base_model_labels = np.reshape(labels_arr, (30735,1))\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "flatten = tf.keras.layers.Flatten()\n",
    "fc_layer1 = tf.keras.layers.Dense(units=1024, activation = 'relu')\n",
    "fc_layer2 = tf.keras.layers.Dense(units=768, activation = 'relu')\n",
    "fc_layer3 = tf.keras.layers.Dense(units=512, activation = 'relu')\n",
    "fc_layer4 = tf.keras.layers.Dense(units=256, activation = 'relu')\n",
    "fc_layer5 = tf.keras.layers.Dense(units=128, activation = 'relu')\n",
    "fc_layer6 = tf.keras.layers.Dense(units=64, activation = 'relu')\n",
    "fc_layer7 = tf.keras.layers.Dense(units=32, activation = 'relu')\n",
    "fc_layer8 = tf.keras.layers.Dense(units=1, activation = 'sigmoid')\n",
    "# Cant use softmax at the end since it will normalize and give 1 \n",
    "base_model = tf.keras.Sequential([\n",
    "    flatten,\n",
    " fc_layer1, fc_layer2, \n",
    "fc_layer3, \n",
    "fc_layer4,\n",
    "fc_layer5,\n",
    "fc_layer6,\n",
    "fc_layer7,\n",
    "fc_layer8\n",
    "])\n",
    "\n",
    "base_model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits = False), \\\n",
    "                   optimizer=tf.keras.optimizers.Adam(0.001, beta_1=0.9, beta_2= 0.999), \\\n",
    "                   metrics=[tf.keras.metrics.Accuracy()]) \n",
    "\n",
    "base_model.build((30735, 200, 3))\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.6291 - accuracy: 0.0000e+00\n",
      "Epoch 2/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.5823 - accuracy: 0.0000e+00\n",
      "Epoch 3/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.5244 - accuracy: 0.0000e+00\n",
      "Epoch 4/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.4196 - accuracy: 5.2058e-04\n",
      "Epoch 5/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.3106 - accuracy: 0.0102\n",
      "Epoch 6/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.2346 - accuracy: 0.0243\n",
      "Epoch 7/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.1966 - accuracy: 0.0427\n",
      "Epoch 8/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.1768 - accuracy: 0.0588\n",
      "Epoch 9/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.1674 - accuracy: 0.0719\n",
      "Epoch 10/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.1540 - accuracy: 0.0871\n",
      "Epoch 11/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.1367 - accuracy: 0.1157\n",
      "Epoch 12/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.1217 - accuracy: 0.1177\n",
      "Epoch 13/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.1232 - accuracy: 0.1404\n",
      "Epoch 14/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.1114 - accuracy: 0.1870\n",
      "Epoch 15/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.1098 - accuracy: 0.1324\n",
      "Epoch 16/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.1111 - accuracy: 0.1154\n",
      "Epoch 17/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.1033 - accuracy: 0.1287\n",
      "Epoch 18/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0976 - accuracy: 0.1502\n",
      "Epoch 19/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0968 - accuracy: 0.1441\n",
      "Epoch 20/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0934 - accuracy: 0.1890\n",
      "Epoch 21/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0887 - accuracy: 0.1935\n",
      "Epoch 22/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.1001 - accuracy: 0.1923\n",
      "Epoch 23/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0912 - accuracy: 0.1357\n",
      "Epoch 24/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0826 - accuracy: 0.1935\n",
      "Epoch 25/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0847 - accuracy: 0.2050\n",
      "Epoch 26/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0873 - accuracy: 0.2616\n",
      "Epoch 27/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0784 - accuracy: 0.2107\n",
      "Epoch 28/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0667 - accuracy: 0.2828\n",
      "Epoch 29/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0758 - accuracy: 0.2538\n",
      "Epoch 30/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0725 - accuracy: 0.1783\n",
      "Epoch 31/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0792 - accuracy: 0.2336\n",
      "Epoch 32/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0745 - accuracy: 0.2188\n",
      "Epoch 33/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0635 - accuracy: 0.2391\n",
      "Epoch 34/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0633 - accuracy: 0.2853\n",
      "Epoch 35/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0658 - accuracy: 0.2862\n",
      "Epoch 36/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0701 - accuracy: 0.2044\n",
      "Epoch 37/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0615 - accuracy: 0.2264\n",
      "Epoch 38/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0596 - accuracy: 0.2669\n",
      "Epoch 39/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0618 - accuracy: 0.2528\n",
      "Epoch 40/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0586 - accuracy: 0.3160\n",
      "Epoch 41/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0589 - accuracy: 0.2489\n",
      "Epoch 42/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0687 - accuracy: 0.2110\n",
      "Epoch 43/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0626 - accuracy: 0.2163\n",
      "Epoch 44/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0576 - accuracy: 0.2052\n",
      "Epoch 45/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0629 - accuracy: 0.2062\n",
      "Epoch 46/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0655 - accuracy: 0.2805\n",
      "Epoch 47/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0667 - accuracy: 0.3361\n",
      "Epoch 48/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0551 - accuracy: 0.3022\n",
      "Epoch 49/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0594 - accuracy: 0.3422\n",
      "Epoch 50/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0499 - accuracy: 0.3533\n",
      "Epoch 51/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0519 - accuracy: 0.3791\n",
      "Epoch 52/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0519 - accuracy: 0.3095\n",
      "Epoch 53/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0512 - accuracy: 0.2407\n",
      "Epoch 54/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0496 - accuracy: 0.3614\n",
      "Epoch 55/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0491 - accuracy: 0.3646\n",
      "Epoch 56/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0488 - accuracy: 0.4129\n",
      "Epoch 57/1000\n",
      "121/121 [==============================] - 2s 16ms/step - loss: 0.0437 - accuracy: 0.4279\n",
      "Epoch 58/1000\n",
      "121/121 [==============================] - 2s 19ms/step - loss: 0.0541 - accuracy: 0.3047\n",
      "Epoch 59/1000\n",
      "121/121 [==============================] - 2s 17ms/step - loss: 0.0471 - accuracy: 0.3435\n",
      "Epoch 60/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0522 - accuracy: 0.3555\n",
      "Epoch 61/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0808 - accuracy: 0.3569\n",
      "Epoch 62/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0584 - accuracy: 0.2822\n",
      "Epoch 63/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0503 - accuracy: 0.3534\n",
      "Epoch 64/1000\n",
      "121/121 [==============================] - 2s 17ms/step - loss: 0.0597 - accuracy: 0.2891\n",
      "Epoch 65/1000\n",
      "121/121 [==============================] - 2s 17ms/step - loss: 0.0485 - accuracy: 0.3300\n",
      "Epoch 66/1000\n",
      "121/121 [==============================] - 2s 18ms/step - loss: 0.0492 - accuracy: 0.3292\n",
      "Epoch 67/1000\n",
      "121/121 [==============================] - 2s 17ms/step - loss: 0.0455 - accuracy: 0.2958\n",
      "Epoch 68/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0468 - accuracy: 0.3938\n",
      "Epoch 69/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0423 - accuracy: 0.4126\n",
      "Epoch 70/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0431 - accuracy: 0.3838\n",
      "Epoch 71/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0417 - accuracy: 0.4164\n",
      "Epoch 72/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0409 - accuracy: 0.3865\n",
      "Epoch 73/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0398 - accuracy: 0.3706\n",
      "Epoch 74/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0473 - accuracy: 0.4042\n",
      "Epoch 75/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0435 - accuracy: 0.4048\n",
      "Epoch 76/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0437 - accuracy: 0.3701\n",
      "Epoch 77/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0438 - accuracy: 0.3280\n",
      "Epoch 78/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0443 - accuracy: 0.3120\n",
      "Epoch 79/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0525 - accuracy: 0.3505\n",
      "Epoch 80/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0424 - accuracy: 0.3760\n",
      "Epoch 81/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0564 - accuracy: 0.3071\n",
      "Epoch 82/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0358 - accuracy: 0.3430\n",
      "Epoch 83/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0370 - accuracy: 0.3766\n",
      "Epoch 84/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0415 - accuracy: 0.3469\n",
      "Epoch 85/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0340 - accuracy: 0.3722\n",
      "Epoch 86/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0455 - accuracy: 0.3694\n",
      "Epoch 87/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0417 - accuracy: 0.3147\n",
      "Epoch 88/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0392 - accuracy: 0.4035\n",
      "Epoch 89/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0515 - accuracy: 0.2726\n",
      "Epoch 90/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0371 - accuracy: 0.3666\n",
      "Epoch 91/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0292 - accuracy: 0.4330\n",
      "Epoch 92/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0313 - accuracy: 0.4078\n",
      "Epoch 93/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0361 - accuracy: 0.4484\n",
      "Epoch 94/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0334 - accuracy: 0.4378\n",
      "Epoch 95/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0313 - accuracy: 0.4657\n",
      "Epoch 96/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0496 - accuracy: 0.4329\n",
      "Epoch 97/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0364 - accuracy: 0.4803\n",
      "Epoch 98/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0314 - accuracy: 0.4829\n",
      "Epoch 99/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0286 - accuracy: 0.4938\n",
      "Epoch 100/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0280 - accuracy: 0.4978\n",
      "Epoch 101/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0292 - accuracy: 0.5086\n",
      "Epoch 102/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0277 - accuracy: 0.5203\n",
      "Epoch 103/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0273 - accuracy: 0.5256\n",
      "Epoch 104/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0320 - accuracy: 0.5292\n",
      "Epoch 105/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0252 - accuracy: 0.5380\n",
      "Epoch 106/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0277 - accuracy: 0.5370\n",
      "Epoch 107/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0447 - accuracy: 0.3505\n",
      "Epoch 108/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0478 - accuracy: 0.2534\n",
      "Epoch 109/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0476 - accuracy: 0.2695\n",
      "Epoch 110/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0313 - accuracy: 0.3479\n",
      "Epoch 111/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0317 - accuracy: 0.4021\n",
      "Epoch 112/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0305 - accuracy: 0.4646\n",
      "Epoch 113/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0299 - accuracy: 0.4857\n",
      "Epoch 114/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0267 - accuracy: 0.4967\n",
      "Epoch 115/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0254 - accuracy: 0.5096\n",
      "Epoch 116/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0617 - accuracy: 0.4858\n",
      "Epoch 117/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0351 - accuracy: 0.4269\n",
      "Epoch 118/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0398 - accuracy: 0.3961\n",
      "Epoch 119/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0309 - accuracy: 0.3719\n",
      "Epoch 120/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0395 - accuracy: 0.3497\n",
      "Epoch 121/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0626 - accuracy: 0.3185\n",
      "Epoch 122/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0341 - accuracy: 0.3385\n",
      "Epoch 123/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0358 - accuracy: 0.3939\n",
      "Epoch 124/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0238 - accuracy: 0.4706\n",
      "Epoch 125/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0244 - accuracy: 0.5222\n",
      "Epoch 126/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0245 - accuracy: 0.5228\n",
      "Epoch 127/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0247 - accuracy: 0.5367\n",
      "Epoch 128/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0263 - accuracy: 0.5349\n",
      "Epoch 129/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0213 - accuracy: 0.5460\n",
      "Epoch 130/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0233 - accuracy: 0.5037\n",
      "Epoch 131/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0311 - accuracy: 0.3922\n",
      "Epoch 132/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0295 - accuracy: 0.4196\n",
      "Epoch 133/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0227 - accuracy: 0.4067\n",
      "Epoch 134/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0245 - accuracy: 0.4376\n",
      "Epoch 135/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0219 - accuracy: 0.4808\n",
      "Epoch 136/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0217 - accuracy: 0.4810\n",
      "Epoch 137/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0257 - accuracy: 0.4738\n",
      "Epoch 138/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0204 - accuracy: 0.4869\n",
      "Epoch 139/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0239 - accuracy: 0.4797\n",
      "Epoch 140/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0251 - accuracy: 0.4772\n",
      "Epoch 141/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0210 - accuracy: 0.4999\n",
      "Epoch 142/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0229 - accuracy: 0.4846\n",
      "Epoch 143/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0181 - accuracy: 0.5000\n",
      "Epoch 144/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0172 - accuracy: 0.4929\n",
      "Epoch 145/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0198 - accuracy: 0.4783\n",
      "Epoch 146/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0158 - accuracy: 0.5155\n",
      "Epoch 147/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0265 - accuracy: 0.4930\n",
      "Epoch 148/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0176 - accuracy: 0.5014\n",
      "Epoch 149/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0248 - accuracy: 0.5081\n",
      "Epoch 150/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0205 - accuracy: 0.5069\n",
      "Epoch 151/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0214 - accuracy: 0.5304\n",
      "Epoch 152/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0140 - accuracy: 0.5506\n",
      "Epoch 153/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0385 - accuracy: 0.3766\n",
      "Epoch 154/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0419 - accuracy: 0.2013\n",
      "Epoch 155/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0302 - accuracy: 0.3119\n",
      "Epoch 156/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0222 - accuracy: 0.3168\n",
      "Epoch 157/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0321 - accuracy: 0.3349\n",
      "Epoch 158/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0187 - accuracy: 0.3929\n",
      "Epoch 159/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0153 - accuracy: 0.4515\n",
      "Epoch 160/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0142 - accuracy: 0.4844\n",
      "Epoch 161/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0256 - accuracy: 0.4600\n",
      "Epoch 162/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0154 - accuracy: 0.4958\n",
      "Epoch 163/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0247 - accuracy: 0.4622\n",
      "Epoch 164/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0199 - accuracy: 0.4566\n",
      "Epoch 165/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0138 - accuracy: 0.4744\n",
      "Epoch 166/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0156 - accuracy: 0.4926\n",
      "Epoch 167/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0356 - accuracy: 0.4596\n",
      "Epoch 168/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0147 - accuracy: 0.5301\n",
      "Epoch 169/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0138 - accuracy: 0.5322\n",
      "Epoch 170/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0159 - accuracy: 0.5364\n",
      "Epoch 171/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0181 - accuracy: 0.5425\n",
      "Epoch 172/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0174 - accuracy: 0.5573\n",
      "Epoch 173/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0154 - accuracy: 0.5559\n",
      "Epoch 174/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0160 - accuracy: 0.5735\n",
      "Epoch 175/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0176 - accuracy: 0.5573\n",
      "Epoch 176/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0213 - accuracy: 0.5465\n",
      "Epoch 177/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0186 - accuracy: 0.5188\n",
      "Epoch 178/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0131 - accuracy: 0.5625\n",
      "Epoch 179/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0178 - accuracy: 0.5542\n",
      "Epoch 180/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0081 - accuracy: 0.5799\n",
      "Epoch 181/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0101 - accuracy: 0.5952\n",
      "Epoch 182/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0173 - accuracy: 0.5568\n",
      "Epoch 183/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0175 - accuracy: 0.5588\n",
      "Epoch 184/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0103 - accuracy: 0.5631\n",
      "Epoch 185/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0375 - accuracy: 0.5393\n",
      "Epoch 186/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0361 - accuracy: 0.4547\n",
      "Epoch 187/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0446 - accuracy: 0.2964\n",
      "Epoch 188/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0525 - accuracy: 0.3508\n",
      "Epoch 189/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0262 - accuracy: 0.4056\n",
      "Epoch 190/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0230 - accuracy: 0.3636\n",
      "Epoch 191/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0180 - accuracy: 0.4819\n",
      "Epoch 192/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0210 - accuracy: 0.5075\n",
      "Epoch 193/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0113 - accuracy: 0.5594\n",
      "Epoch 194/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0192 - accuracy: 0.5528\n",
      "Epoch 195/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0194 - accuracy: 0.5160\n",
      "Epoch 196/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0136 - accuracy: 0.5587\n",
      "Epoch 197/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0231 - accuracy: 0.5435\n",
      "Epoch 198/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0150 - accuracy: 0.5699\n",
      "Epoch 199/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0150 - accuracy: 0.5815\n",
      "Epoch 200/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0114 - accuracy: 0.5868\n",
      "Epoch 201/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0123 - accuracy: 0.6044\n",
      "Epoch 202/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0155 - accuracy: 0.5707\n",
      "Epoch 203/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0081 - accuracy: 0.5846\n",
      "Epoch 204/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0153 - accuracy: 0.5802\n",
      "Epoch 205/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0231 - accuracy: 0.4547\n",
      "Epoch 206/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0165 - accuracy: 0.4080\n",
      "Epoch 207/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0148 - accuracy: 0.4779\n",
      "Epoch 208/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0220 - accuracy: 0.4336\n",
      "Epoch 209/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0165 - accuracy: 0.4348\n",
      "Epoch 210/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0127 - accuracy: 0.5008\n",
      "Epoch 211/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0178 - accuracy: 0.4094\n",
      "Epoch 212/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0103 - accuracy: 0.5121\n",
      "Epoch 213/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0210 - accuracy: 0.4442\n",
      "Epoch 214/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0149 - accuracy: 0.4986\n",
      "Epoch 215/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0121 - accuracy: 0.5419\n",
      "Epoch 216/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0246 - accuracy: 0.5165\n",
      "Epoch 217/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0159 - accuracy: 0.5517\n",
      "Epoch 218/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0076 - accuracy: 0.5903\n",
      "Epoch 219/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0074 - accuracy: 0.5971\n",
      "Epoch 220/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0174 - accuracy: 0.5838\n",
      "Epoch 221/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0129 - accuracy: 0.5765\n",
      "Epoch 222/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0088 - accuracy: 0.5925\n",
      "Epoch 223/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0124 - accuracy: 0.5833\n",
      "Epoch 224/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0098 - accuracy: 0.5975\n",
      "Epoch 225/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0187 - accuracy: 0.5899\n",
      "Epoch 226/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0152 - accuracy: 0.5973\n",
      "Epoch 227/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0098 - accuracy: 0.6063\n",
      "Epoch 228/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0185 - accuracy: 0.5944\n",
      "Epoch 229/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0108 - accuracy: 0.6096\n",
      "Epoch 230/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0516 - accuracy: 0.5792\n",
      "Epoch 231/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0328 - accuracy: 0.5212\n",
      "Epoch 232/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0487 - accuracy: 0.2993\n",
      "Epoch 233/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0251 - accuracy: 0.3881\n",
      "Epoch 234/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0319 - accuracy: 0.2924\n",
      "Epoch 235/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0244 - accuracy: 0.4761\n",
      "Epoch 236/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0132 - accuracy: 0.5123\n",
      "Epoch 237/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0145 - accuracy: 0.5233\n",
      "Epoch 238/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0120 - accuracy: 0.5371\n",
      "Epoch 239/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0109 - accuracy: 0.5539\n",
      "Epoch 240/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0114 - accuracy: 0.5558\n",
      "Epoch 241/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0111 - accuracy: 0.5714\n",
      "Epoch 242/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0111 - accuracy: 0.5518\n",
      "Epoch 243/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0161 - accuracy: 0.5330\n",
      "Epoch 244/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0163 - accuracy: 0.5272\n",
      "Epoch 245/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0119 - accuracy: 0.5308\n",
      "Epoch 246/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0115 - accuracy: 0.5377\n",
      "Epoch 247/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0091 - accuracy: 0.5521\n",
      "Epoch 248/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0084 - accuracy: 0.5592\n",
      "Epoch 249/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0098 - accuracy: 0.5566\n",
      "Epoch 250/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0114 - accuracy: 0.5560\n",
      "Epoch 251/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0096 - accuracy: 0.5534\n",
      "Epoch 252/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0142 - accuracy: 0.5372\n",
      "Epoch 253/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0066 - accuracy: 0.5702\n",
      "Epoch 254/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0067 - accuracy: 0.5838\n",
      "Epoch 255/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0130 - accuracy: 0.5690\n",
      "Epoch 256/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0065 - accuracy: 0.5530\n",
      "Epoch 257/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0073 - accuracy: 0.5820\n",
      "Epoch 258/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0086 - accuracy: 0.5775\n",
      "Epoch 259/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0203 - accuracy: 0.5395\n",
      "Epoch 260/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0163 - accuracy: 0.4156\n",
      "Epoch 261/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0193 - accuracy: 0.4223\n",
      "Epoch 262/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0286 - accuracy: 0.3713\n",
      "Epoch 263/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0275 - accuracy: 0.3921\n",
      "Epoch 264/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0122 - accuracy: 0.4981\n",
      "Epoch 265/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0143 - accuracy: 0.4369\n",
      "Epoch 266/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0181 - accuracy: 0.4926\n",
      "Epoch 267/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0093 - accuracy: 0.5401\n",
      "Epoch 268/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0047 - accuracy: 0.5816\n",
      "Epoch 269/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0205 - accuracy: 0.5463\n",
      "Epoch 270/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0040 - accuracy: 0.6116\n",
      "Epoch 271/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0091 - accuracy: 0.6371\n",
      "Epoch 272/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0107 - accuracy: 0.5673\n",
      "Epoch 273/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0063 - accuracy: 0.6097\n",
      "Epoch 274/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0054 - accuracy: 0.6223\n",
      "Epoch 275/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0553 - accuracy: 0.5273\n",
      "Epoch 276/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0275 - accuracy: 0.5588\n",
      "Epoch 277/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0265 - accuracy: 0.5809\n",
      "Epoch 278/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0219 - accuracy: 0.5857\n",
      "Epoch 279/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0213 - accuracy: 0.6001\n",
      "Epoch 280/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0293 - accuracy: 0.6009\n",
      "Epoch 281/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0206 - accuracy: 0.6193\n",
      "Epoch 282/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0462 - accuracy: 0.5008\n",
      "Epoch 283/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0307 - accuracy: 0.3475\n",
      "Epoch 284/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0188 - accuracy: 0.4265\n",
      "Epoch 285/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0096 - accuracy: 0.4320\n",
      "Epoch 286/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0115 - accuracy: 0.4941\n",
      "Epoch 287/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0126 - accuracy: 0.5289\n",
      "Epoch 288/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0153 - accuracy: 0.5580\n",
      "Epoch 289/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0074 - accuracy: 0.5736\n",
      "Epoch 290/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0096 - accuracy: 0.5630\n",
      "Epoch 291/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0154 - accuracy: 0.5690\n",
      "Epoch 292/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0072 - accuracy: 0.6253\n",
      "Epoch 293/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0102 - accuracy: 0.6000\n",
      "Epoch 294/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0096 - accuracy: 0.6079\n",
      "Epoch 295/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0065 - accuracy: 0.6044\n",
      "Epoch 296/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0077 - accuracy: 0.6193\n",
      "Epoch 297/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0081 - accuracy: 0.5973\n",
      "Epoch 298/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0099 - accuracy: 0.5743\n",
      "Epoch 299/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0240 - accuracy: 0.4343\n",
      "Epoch 300/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0184 - accuracy: 0.3997\n",
      "Epoch 301/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0104 - accuracy: 0.4608\n",
      "Epoch 302/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0198 - accuracy: 0.4660\n",
      "Epoch 303/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0064 - accuracy: 0.5654\n",
      "Epoch 304/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0123 - accuracy: 0.4733\n",
      "Epoch 305/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0076 - accuracy: 0.4999\n",
      "Epoch 306/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0067 - accuracy: 0.5170\n",
      "Epoch 307/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0091 - accuracy: 0.5285\n",
      "Epoch 308/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0054 - accuracy: 0.5385\n",
      "Epoch 309/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0032 - accuracy: 0.5603\n",
      "Epoch 310/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0097 - accuracy: 0.5580\n",
      "Epoch 311/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0053 - accuracy: 0.5450\n",
      "Epoch 312/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0027 - accuracy: 0.5721\n",
      "Epoch 313/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0069 - accuracy: 0.5934\n",
      "Epoch 314/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0098 - accuracy: 0.5304\n",
      "Epoch 315/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0239 - accuracy: 0.5085\n",
      "Epoch 316/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0092 - accuracy: 0.5102\n",
      "Epoch 317/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0100 - accuracy: 0.5458\n",
      "Epoch 318/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0064 - accuracy: 0.5386\n",
      "Epoch 319/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0052 - accuracy: 0.5540\n",
      "Epoch 320/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0043 - accuracy: 0.5593\n",
      "Epoch 321/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0075 - accuracy: 0.5592\n",
      "Epoch 322/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0058 - accuracy: 0.5521\n",
      "Epoch 323/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0181 - accuracy: 0.5588\n",
      "Epoch 324/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0041 - accuracy: 0.6115\n",
      "Epoch 325/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0159 - accuracy: 0.5805\n",
      "Epoch 326/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0066 - accuracy: 0.6283\n",
      "Epoch 327/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0195 - accuracy: 0.5862\n",
      "Epoch 328/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0088 - accuracy: 0.5649\n",
      "Epoch 329/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0028 - accuracy: 0.5959\n",
      "Epoch 330/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0082 - accuracy: 0.5990\n",
      "Epoch 331/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0153 - accuracy: 0.5855\n",
      "Epoch 332/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0055 - accuracy: 0.5906\n",
      "Epoch 333/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0047 - accuracy: 0.6183\n",
      "Epoch 334/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0070 - accuracy: 0.6232\n",
      "Epoch 335/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0065 - accuracy: 0.6367\n",
      "Epoch 336/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0047 - accuracy: 0.6389\n",
      "Epoch 337/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0039 - accuracy: 0.6608\n",
      "Epoch 338/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0019 - accuracy: 0.6806\n",
      "Epoch 339/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0016 - accuracy: 0.6969\n",
      "Epoch 340/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0122 - accuracy: 0.6445\n",
      "Epoch 341/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0030 - accuracy: 0.6234\n",
      "Epoch 342/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0022 - accuracy: 0.6462\n",
      "Epoch 343/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0018 - accuracy: 0.6629\n",
      "Epoch 344/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0226 - accuracy: 0.6255\n",
      "Epoch 345/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0079 - accuracy: 0.6308\n",
      "Epoch 346/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0147 - accuracy: 0.6392\n",
      "Epoch 347/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0098 - accuracy: 0.6020\n",
      "Epoch 348/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0102 - accuracy: 0.6173\n",
      "Epoch 349/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0252 - accuracy: 0.5554\n",
      "Epoch 350/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0484 - accuracy: 0.2779\n",
      "Epoch 351/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0099 - accuracy: 0.4351\n",
      "Epoch 352/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0078 - accuracy: 0.5084\n",
      "Epoch 353/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0098 - accuracy: 0.5655\n",
      "Epoch 354/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0107 - accuracy: 0.5417\n",
      "Epoch 355/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0055 - accuracy: 0.6085\n",
      "Epoch 356/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0081 - accuracy: 0.6091\n",
      "Epoch 357/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0041 - accuracy: 0.6249\n",
      "Epoch 358/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0017 - accuracy: 0.6695\n",
      "Epoch 359/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0021 - accuracy: 0.6930\n",
      "Epoch 360/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0094 - accuracy: 0.6561\n",
      "Epoch 361/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0026 - accuracy: 0.6255\n",
      "Epoch 362/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0017 - accuracy: 0.6456\n",
      "Epoch 363/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0035 - accuracy: 0.6521\n",
      "Epoch 364/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0028 - accuracy: 0.6506\n",
      "Epoch 365/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0096 - accuracy: 0.6246\n",
      "Epoch 366/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0024 - accuracy: 0.6423\n",
      "Epoch 367/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0044 - accuracy: 0.6674\n",
      "Epoch 368/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0075 - accuracy: 0.5947\n",
      "Epoch 369/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0050 - accuracy: 0.6261\n",
      "Epoch 370/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0029 - accuracy: 0.6257\n",
      "Epoch 371/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0109 - accuracy: 0.5760\n",
      "Epoch 372/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0031 - accuracy: 0.5749\n",
      "Epoch 373/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0068 - accuracy: 0.6250\n",
      "Epoch 374/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0107 - accuracy: 0.5235\n",
      "Epoch 375/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0101 - accuracy: 0.5407\n",
      "Epoch 376/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0169 - accuracy: 0.4878\n",
      "Epoch 377/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0091 - accuracy: 0.4543\n",
      "Epoch 378/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0038 - accuracy: 0.5101\n",
      "Epoch 379/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0066 - accuracy: 0.5644\n",
      "Epoch 380/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0027 - accuracy: 0.5977\n",
      "Epoch 381/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0052 - accuracy: 0.6317\n",
      "Epoch 382/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0055 - accuracy: 0.5755\n",
      "Epoch 383/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0029 - accuracy: 0.5218\n",
      "Epoch 384/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0079 - accuracy: 0.5635\n",
      "Epoch 385/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0055 - accuracy: 0.5393\n",
      "Epoch 386/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0163 - accuracy: 0.5548\n",
      "Epoch 387/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0013 - accuracy: 0.6254\n",
      "Epoch 388/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0072 - accuracy: 0.6620\n",
      "Epoch 389/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0028 - accuracy: 0.6347\n",
      "Epoch 390/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 5.7883e-04 - accuracy: 0.6836\n",
      "Epoch 391/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0142 - accuracy: 0.6388\n",
      "Epoch 392/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0023 - accuracy: 0.6357\n",
      "Epoch 393/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 8.6700e-04 - accuracy: 0.6814\n",
      "Epoch 394/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0067 - accuracy: 0.6747\n",
      "Epoch 395/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0021 - accuracy: 0.6849\n",
      "Epoch 396/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 3.1443e-04 - accuracy: 0.7444\n",
      "Epoch 397/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0020 - accuracy: 0.7568\n",
      "Epoch 398/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0133 - accuracy: 0.6558\n",
      "Epoch 399/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0032 - accuracy: 0.6604\n",
      "Epoch 400/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 6.0919e-04 - accuracy: 0.6803\n",
      "Epoch 401/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0028 - accuracy: 0.6884\n",
      "Epoch 402/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 6.3057e-04 - accuracy: 0.6947\n",
      "Epoch 403/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 3.1354e-04 - accuracy: 0.7078\n",
      "Epoch 404/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0080 - accuracy: 0.7118\n",
      "Epoch 405/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0085 - accuracy: 0.6425\n",
      "Epoch 406/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0031 - accuracy: 0.6878\n",
      "Epoch 407/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0156 - accuracy: 0.6684\n",
      "Epoch 408/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0024 - accuracy: 0.6796\n",
      "Epoch 409/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 4.6843e-04 - accuracy: 0.6987\n",
      "Epoch 410/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0061 - accuracy: 0.6896\n",
      "Epoch 411/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0020 - accuracy: 0.6789\n",
      "Epoch 412/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0015 - accuracy: 0.6965\n",
      "Epoch 413/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0055 - accuracy: 0.6941\n",
      "Epoch 414/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 3.5946e-04 - accuracy: 0.6995\n",
      "Epoch 415/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 1.5970e-04 - accuracy: 0.7085\n",
      "Epoch 416/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 1.1735e-04 - accuracy: 0.7202\n",
      "Epoch 417/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0084 - accuracy: 0.6947\n",
      "Epoch 418/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0048 - accuracy: 0.6939\n",
      "Epoch 419/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0046 - accuracy: 0.6593\n",
      "Epoch 420/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0084 - accuracy: 0.6029\n",
      "Epoch 421/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0464 - accuracy: 0.3981\n",
      "Epoch 422/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0329 - accuracy: 0.4024\n",
      "Epoch 423/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0072 - accuracy: 0.4855\n",
      "Epoch 424/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0098 - accuracy: 0.5377\n",
      "Epoch 425/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0021 - accuracy: 0.5757\n",
      "Epoch 426/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0054 - accuracy: 0.5989\n",
      "Epoch 427/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0065 - accuracy: 0.5805\n",
      "Epoch 428/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 4.3562e-04 - accuracy: 0.6083\n",
      "Epoch 429/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 2.1114e-04 - accuracy: 0.6306\n",
      "Epoch 430/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 6.5137e-05 - accuracy: 0.6448\n",
      "Epoch 431/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 4.1445e-05 - accuracy: 0.6560\n",
      "Epoch 432/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 3.2127e-05 - accuracy: 0.6734\n",
      "Epoch 433/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 1.6524e-05 - accuracy: 0.6905\n",
      "Epoch 434/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 1.0575e-05 - accuracy: 0.7004\n",
      "Epoch 435/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 7.4959e-06 - accuracy: 0.7076\n",
      "Epoch 436/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 5.3830e-06 - accuracy: 0.7163\n",
      "Epoch 437/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 4.0146e-06 - accuracy: 0.7248\n",
      "Epoch 438/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 3.1099e-06 - accuracy: 0.7333\n",
      "Epoch 439/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 2.3995e-06 - accuracy: 0.7419\n",
      "Epoch 440/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 1.9693e-06 - accuracy: 0.7506\n",
      "Epoch 441/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 1.6165e-06 - accuracy: 0.7573\n",
      "Epoch 442/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 1.3235e-06 - accuracy: 0.7628\n",
      "Epoch 443/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 1.1192e-06 - accuracy: 0.7670\n",
      "Epoch 444/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 9.4657e-07 - accuracy: 0.7708\n",
      "Epoch 445/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 8.1743e-07 - accuracy: 0.7737\n",
      "Epoch 446/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 7.1270e-07 - accuracy: 0.7763\n",
      "Epoch 447/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 6.2550e-07 - accuracy: 0.7788\n",
      "Epoch 448/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 5.4992e-07 - accuracy: 0.7806\n",
      "Epoch 449/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 4.8670e-07 - accuracy: 0.7823\n",
      "Epoch 450/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 4.3546e-07 - accuracy: 0.7838\n",
      "Epoch 451/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 3.8667e-07 - accuracy: 0.7854\n",
      "Epoch 452/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 3.4667e-07 - accuracy: 0.7867\n",
      "Epoch 453/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 3.1226e-07 - accuracy: 0.7883\n",
      "Epoch 454/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 2.7995e-07 - accuracy: 0.7893\n",
      "Epoch 455/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 2.5294e-07 - accuracy: 0.7905\n",
      "Epoch 456/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 2.2808e-07 - accuracy: 0.7918\n",
      "Epoch 457/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 2.0910e-07 - accuracy: 0.7924\n",
      "Epoch 458/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 1.8983e-07 - accuracy: 0.7934\n",
      "Epoch 459/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 1.7326e-07 - accuracy: 0.7943\n",
      "Epoch 460/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 1.5715e-07 - accuracy: 0.7947\n",
      "Epoch 461/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 1.4477e-07 - accuracy: 0.7955\n",
      "Epoch 462/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 1.3177e-07 - accuracy: 0.7966\n",
      "Epoch 463/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 1.2248e-07 - accuracy: 0.7972\n",
      "Epoch 464/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 1.1347e-07 - accuracy: 0.7980\n",
      "Epoch 465/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 1.0536e-07 - accuracy: 0.7983\n",
      "Epoch 466/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 9.5033e-08 - accuracy: 0.7987\n",
      "Epoch 467/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 9.0171e-08 - accuracy: 0.7997\n",
      "Epoch 468/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 8.2470e-08 - accuracy: 0.8001\n",
      "Epoch 469/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 7.5579e-08 - accuracy: 0.8005\n",
      "Epoch 470/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 7.2627e-08 - accuracy: 0.8008\n",
      "Epoch 471/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 6.6152e-08 - accuracy: 0.8015\n",
      "Epoch 472/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 6.1839e-08 - accuracy: 0.8017\n",
      "Epoch 473/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 5.7587e-08 - accuracy: 0.8023\n",
      "Epoch 474/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 5.3358e-08 - accuracy: 0.8029\n",
      "Epoch 475/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 5.0654e-08 - accuracy: 0.8034\n",
      "Epoch 476/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 4.6148e-08 - accuracy: 0.8036\n",
      "Epoch 477/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 4.3150e-08 - accuracy: 0.8038\n",
      "Epoch 478/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 4.0538e-08 - accuracy: 0.8044\n",
      "Epoch 479/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 4.0492e-08 - accuracy: 0.8039\n",
      "Epoch 480/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 3.6972e-08 - accuracy: 0.8041\n",
      "Epoch 481/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 3.3598e-08 - accuracy: 0.8048\n",
      "Epoch 482/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 3.1959e-08 - accuracy: 0.8052\n",
      "Epoch 483/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 3.0258e-08 - accuracy: 0.8058\n",
      "Epoch 484/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 2.9307e-08 - accuracy: 0.8057\n",
      "Epoch 485/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 2.6095e-08 - accuracy: 0.8060\n",
      "Epoch 486/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 2.4137e-08 - accuracy: 0.8064\n",
      "Epoch 487/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 2.2770e-08 - accuracy: 0.8071\n",
      "Epoch 488/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 2.2112e-08 - accuracy: 0.8075\n",
      "Epoch 489/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 1.9834e-08 - accuracy: 0.8078\n",
      "Epoch 490/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 1.8859e-08 - accuracy: 0.8080\n",
      "Epoch 491/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 1.7646e-08 - accuracy: 0.8083\n",
      "Epoch 492/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 1.6729e-08 - accuracy: 0.8089\n",
      "Epoch 493/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 1.5328e-08 - accuracy: 0.8089\n",
      "Epoch 494/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 1.4598e-08 - accuracy: 0.8092\n",
      "Epoch 495/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 1.3716e-08 - accuracy: 0.8096\n",
      "Epoch 496/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 1.3014e-08 - accuracy: 0.8097\n",
      "Epoch 497/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 1.2253e-08 - accuracy: 0.8098\n",
      "Epoch 498/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 1.1474e-08 - accuracy: 0.8101\n",
      "Epoch 499/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 1.1141e-08 - accuracy: 0.8107\n",
      "Epoch 500/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 1.0256e-08 - accuracy: 0.8110\n",
      "Epoch 501/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 9.7940e-09 - accuracy: 0.8112\n",
      "Epoch 502/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 9.3770e-09 - accuracy: 0.8116\n",
      "Epoch 503/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 8.6722e-09 - accuracy: 0.8118\n",
      "Epoch 504/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 8.3284e-09 - accuracy: 0.8119\n",
      "Epoch 505/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 7.7055e-09 - accuracy: 0.8119\n",
      "Epoch 506/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 7.4255e-09 - accuracy: 0.8122\n",
      "Epoch 507/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 7.0411e-09 - accuracy: 0.8123\n",
      "Epoch 508/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 6.8193e-09 - accuracy: 0.8125\n",
      "Epoch 509/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 6.2910e-09 - accuracy: 0.8127\n",
      "Epoch 510/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 6.2395e-09 - accuracy: 0.8129\n",
      "Epoch 511/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 5.7616e-09 - accuracy: 0.8130\n",
      "Epoch 512/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 5.3656e-09 - accuracy: 0.8132\n",
      "Epoch 513/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 5.2491e-09 - accuracy: 0.8136\n",
      "Epoch 514/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 4.8162e-09 - accuracy: 0.8137\n",
      "Epoch 515/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 4.6342e-09 - accuracy: 0.8135\n",
      "Epoch 516/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 4.3611e-09 - accuracy: 0.8139\n",
      "Epoch 517/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 4.2118e-09 - accuracy: 0.8139\n",
      "Epoch 518/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 4.1524e-09 - accuracy: 0.8140\n",
      "Epoch 519/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 3.7315e-09 - accuracy: 0.8141\n",
      "Epoch 520/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 3.6957e-09 - accuracy: 0.8144\n",
      "Epoch 521/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 3.5539e-09 - accuracy: 0.8144\n",
      "Epoch 522/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 3.3586e-09 - accuracy: 0.8146\n",
      "Epoch 523/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 3.0623e-09 - accuracy: 0.8147\n",
      "Epoch 524/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 2.9417e-09 - accuracy: 0.8148\n",
      "Epoch 525/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 2.8664e-09 - accuracy: 0.8148\n",
      "Epoch 526/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 2.5994e-09 - accuracy: 0.8149\n",
      "Epoch 527/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 2.5146e-09 - accuracy: 0.8150\n",
      "Epoch 528/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 2.6455e-09 - accuracy: 0.8152\n",
      "Epoch 529/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 2.3674e-09 - accuracy: 0.8153\n",
      "Epoch 530/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 2.2281e-09 - accuracy: 0.8156\n",
      "Epoch 531/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 2.1355e-09 - accuracy: 0.8156\n",
      "Epoch 532/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 2.0662e-09 - accuracy: 0.8157\n",
      "Epoch 533/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 2.0204e-09 - accuracy: 0.8157\n",
      "Epoch 534/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 1.9827e-09 - accuracy: 0.8159\n",
      "Epoch 535/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 2.1141e-09 - accuracy: 0.8162\n",
      "Epoch 536/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 1.7166e-09 - accuracy: 0.8161\n",
      "Epoch 537/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 1.5359e-09 - accuracy: 0.8163\n",
      "Epoch 538/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 1.5077e-09 - accuracy: 0.8164\n",
      "Epoch 539/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 1.5511e-09 - accuracy: 0.8164\n",
      "Epoch 540/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 1.4781e-09 - accuracy: 0.8164\n",
      "Epoch 541/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 1.2712e-09 - accuracy: 0.8166\n",
      "Epoch 542/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 1.2198e-09 - accuracy: 0.8166\n",
      "Epoch 543/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 1.2890e-09 - accuracy: 0.8168\n",
      "Epoch 544/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 1.3823e-09 - accuracy: 0.8169\n",
      "Epoch 545/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 1.1984e-09 - accuracy: 0.8169\n",
      "Epoch 546/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 1.0543e-09 - accuracy: 0.8172\n",
      "Epoch 547/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 1.1428e-09 - accuracy: 0.8172\n",
      "Epoch 548/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 9.8013e-10 - accuracy: 0.8172\n",
      "Epoch 549/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 9.6317e-10 - accuracy: 0.8174\n",
      "Epoch 550/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 9.4168e-10 - accuracy: 0.8177\n",
      "Epoch 551/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 8.2630e-10 - accuracy: 0.8179\n",
      "Epoch 552/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 8.8367e-10 - accuracy: 0.8179\n",
      "Epoch 553/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 8.2798e-10 - accuracy: 0.8181\n",
      "Epoch 554/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 7.8125e-10 - accuracy: 0.8182\n",
      "Epoch 555/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 7.5682e-10 - accuracy: 0.8182\n",
      "Epoch 556/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 6.7568e-10 - accuracy: 0.8183\n",
      "Epoch 557/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 7.1377e-10 - accuracy: 0.8183\n",
      "Epoch 558/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 7.2357e-10 - accuracy: 0.8185\n",
      "Epoch 559/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 6.0557e-10 - accuracy: 0.8185\n",
      "Epoch 560/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 6.2249e-10 - accuracy: 0.8185\n",
      "Epoch 561/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 5.5415e-10 - accuracy: 0.8186\n",
      "Epoch 562/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 6.1680e-10 - accuracy: 0.8186\n",
      "Epoch 563/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 5.7763e-10 - accuracy: 0.8187\n",
      "Epoch 564/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 5.2973e-10 - accuracy: 0.8188\n",
      "Epoch 565/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 5.3336e-10 - accuracy: 0.8190\n",
      "Epoch 566/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 4.8064e-10 - accuracy: 0.8190\n",
      "Epoch 567/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 4.9016e-10 - accuracy: 0.8191\n",
      "Epoch 568/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 4.3844e-10 - accuracy: 0.8192\n",
      "Epoch 569/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 4.7636e-10 - accuracy: 0.8192\n",
      "Epoch 570/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 4.4334e-10 - accuracy: 0.8194\n",
      "Epoch 571/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 3.7486e-10 - accuracy: 0.8194\n",
      "Epoch 572/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 4.7340e-10 - accuracy: 0.8195\n",
      "Epoch 573/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 3.8789e-10 - accuracy: 0.8196\n",
      "Epoch 574/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 1.5827e-09 - accuracy: 0.8200\n",
      "Epoch 575/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0731 - accuracy: 0.7664\n",
      "Epoch 576/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.1511 - accuracy: 0.2206\n",
      "Epoch 577/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0267 - accuracy: 0.3614\n",
      "Epoch 578/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0133 - accuracy: 0.4648\n",
      "Epoch 579/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0149 - accuracy: 0.4932\n",
      "Epoch 580/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0091 - accuracy: 0.5487\n",
      "Epoch 581/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0063 - accuracy: 0.5737\n",
      "Epoch 582/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0104 - accuracy: 0.5834\n",
      "Epoch 583/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0017 - accuracy: 0.6044\n",
      "Epoch 584/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0077 - accuracy: 0.6213\n",
      "Epoch 585/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0064 - accuracy: 0.5980\n",
      "Epoch 586/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0093 - accuracy: 0.6259\n",
      "Epoch 587/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0062 - accuracy: 0.5896\n",
      "Epoch 588/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 3.3827e-04 - accuracy: 0.6176\n",
      "Epoch 589/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0126 - accuracy: 0.6098\n",
      "Epoch 590/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0080 - accuracy: 0.6011\n",
      "Epoch 591/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 3.9890e-04 - accuracy: 0.6429\n",
      "Epoch 592/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 1.9002e-04 - accuracy: 0.6689\n",
      "Epoch 593/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 8.3247e-04 - accuracy: 0.6824\n",
      "Epoch 594/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0152 - accuracy: 0.6158\n",
      "Epoch 595/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0021 - accuracy: 0.6269\n",
      "Epoch 596/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 5.5570e-04 - accuracy: 0.6399\n",
      "Epoch 597/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0065 - accuracy: 0.6547\n",
      "Epoch 598/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0035 - accuracy: 0.6251\n",
      "Epoch 599/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 2.6639e-04 - accuracy: 0.6470\n",
      "Epoch 600/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 1.4547e-04 - accuracy: 0.6626\n",
      "Epoch 601/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 3.7904e-05 - accuracy: 0.6740\n",
      "Epoch 602/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 2.6208e-05 - accuracy: 0.6816\n",
      "Epoch 603/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0271 - accuracy: 0.6057\n",
      "Epoch 604/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0173 - accuracy: 0.5817\n",
      "Epoch 605/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0217 - accuracy: 0.6085\n",
      "Epoch 606/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0141 - accuracy: 0.4567\n",
      "Epoch 607/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0038 - accuracy: 0.5348\n",
      "Epoch 608/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0013 - accuracy: 0.5602\n",
      "Epoch 609/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0049 - accuracy: 0.5552\n",
      "Epoch 610/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0176 - accuracy: 0.5811\n",
      "Epoch 611/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0020 - accuracy: 0.5508\n",
      "Epoch 612/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0012 - accuracy: 0.6204\n",
      "Epoch 613/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 9.5474e-05 - accuracy: 0.6501\n",
      "Epoch 614/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 5.0234e-05 - accuracy: 0.6589\n",
      "Epoch 615/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 3.4767e-05 - accuracy: 0.6656\n",
      "Epoch 616/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 2.3752e-05 - accuracy: 0.6750\n",
      "Epoch 617/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 2.1871e-05 - accuracy: 0.6832\n",
      "Epoch 618/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 1.3777e-05 - accuracy: 0.6899\n",
      "Epoch 619/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 1.0594e-05 - accuracy: 0.6954\n",
      "Epoch 620/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 8.6163e-06 - accuracy: 0.6998\n",
      "Epoch 621/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 6.7647e-06 - accuracy: 0.7030\n",
      "Epoch 622/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 5.8746e-06 - accuracy: 0.7069\n",
      "Epoch 623/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 4.9539e-06 - accuracy: 0.7099\n",
      "Epoch 624/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 4.2057e-06 - accuracy: 0.7128\n",
      "Epoch 625/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 4.1872e-06 - accuracy: 0.7154\n",
      "Epoch 626/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 3.1464e-06 - accuracy: 0.7177\n",
      "Epoch 627/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 2.5662e-06 - accuracy: 0.7202\n",
      "Epoch 628/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 2.3170e-06 - accuracy: 0.7234\n",
      "Epoch 629/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 2.0269e-06 - accuracy: 0.7251\n",
      "Epoch 630/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 1.8466e-06 - accuracy: 0.7270\n",
      "Epoch 631/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 1.6240e-06 - accuracy: 0.7287\n",
      "Epoch 632/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 1.4756e-06 - accuracy: 0.7303\n",
      "Epoch 633/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 1.2933e-06 - accuracy: 0.7322\n",
      "Epoch 634/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 1.1891e-06 - accuracy: 0.7338\n",
      "Epoch 635/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 1.1117e-06 - accuracy: 0.7355\n",
      "Epoch 636/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 9.2527e-07 - accuracy: 0.7377\n",
      "Epoch 637/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 8.4595e-07 - accuracy: 0.7389\n",
      "Epoch 638/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 7.6090e-07 - accuracy: 0.7407\n",
      "Epoch 639/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 6.8921e-07 - accuracy: 0.7421\n",
      "Epoch 640/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 6.3738e-07 - accuracy: 0.7435\n",
      "Epoch 641/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 5.4014e-07 - accuracy: 0.7447\n",
      "Epoch 642/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 5.0378e-07 - accuracy: 0.7463\n",
      "Epoch 643/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 4.3990e-07 - accuracy: 0.7473\n",
      "Epoch 644/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 4.0473e-07 - accuracy: 0.7484\n",
      "Epoch 645/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 3.7313e-07 - accuracy: 0.7496\n",
      "Epoch 646/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 3.4354e-07 - accuracy: 0.7507\n",
      "Epoch 647/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 3.0333e-07 - accuracy: 0.7517\n",
      "Epoch 648/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 2.7674e-07 - accuracy: 0.7528\n",
      "Epoch 649/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 2.5295e-07 - accuracy: 0.7538\n",
      "Epoch 650/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 2.3085e-07 - accuracy: 0.7545\n",
      "Epoch 651/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 2.0933e-07 - accuracy: 0.7556\n",
      "Epoch 652/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 2.0061e-07 - accuracy: 0.7563\n",
      "Epoch 653/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 1.7323e-07 - accuracy: 0.7573\n",
      "Epoch 654/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 1.6538e-07 - accuracy: 0.7585\n",
      "Epoch 655/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 1.5195e-07 - accuracy: 0.7588\n",
      "Epoch 656/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 1.3410e-07 - accuracy: 0.7597\n",
      "Epoch 657/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 1.2261e-07 - accuracy: 0.7608\n",
      "Epoch 658/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 1.1284e-07 - accuracy: 0.7612\n",
      "Epoch 659/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 1.0402e-07 - accuracy: 0.7620\n",
      "Epoch 660/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 9.5116e-08 - accuracy: 0.7629\n",
      "Epoch 661/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 8.7855e-08 - accuracy: 0.7633\n",
      "Epoch 662/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 7.9868e-08 - accuracy: 0.7646\n",
      "Epoch 663/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 7.4839e-08 - accuracy: 0.7655\n",
      "Epoch 664/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 6.9640e-08 - accuracy: 0.7662\n",
      "Epoch 665/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 6.2232e-08 - accuracy: 0.7673\n",
      "Epoch 666/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 5.9971e-08 - accuracy: 0.7681\n",
      "Epoch 667/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 5.3813e-08 - accuracy: 0.7691\n",
      "Epoch 668/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 5.0407e-08 - accuracy: 0.7696\n",
      "Epoch 669/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 4.5840e-08 - accuracy: 0.7703\n",
      "Epoch 670/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 4.2929e-08 - accuracy: 0.7713\n",
      "Epoch 671/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 4.0134e-08 - accuracy: 0.7720\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x3149ef250>"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=100)\n",
    "base_model.fit(base_model_data, base_model_labels, epochs = 1000,\n",
    "               batch_size = 256, \n",
    "               verbose=1, \n",
    "               callbacks=[callback]) \n",
    "\n",
    "# Paper baseline model as-it-is\n",
    "#                Epoch 500/500\n",
    "# 961/961 [==============================] - 1s 579us/step - loss: 0.0989 - accuracy: 0.4819"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.], dtype=float32)"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# base_model_data[0:32]\n",
    "base_model(base_model_data[0:1])\n",
    "# base_model_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.6636794931155237e-08, 0.7729299068450928)"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, accuracy = base_model.evaluate(base_model_data, base_model_labels,\n",
    "                                    #  batch_size = batch_size, \n",
    "                                     verbose=0)\n",
    "\n",
    "loss,accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.save('../code/base_model_dense_layers_only.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../code/base_model_dense_layers.pkl/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../code/base_model_dense_layers.pkl/assets\n"
     ]
    }
   ],
   "source": [
    "base_model.save('../code/base_model_dense_layers.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7y/kyw1v_8j0g1ckfb3g6x93q1m0000gn/T/ipykernel_92662/2774257938.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  acc_data_pid['tac'] = acc_data_pid[\"time\"].apply(lambda x: clean_tac_data.iloc[clean_tac_data.index.get_loc(x)][\"TAC_Reading\"])\n"
     ]
    }
   ],
   "source": [
    "# Read clean tac data for pid = BK7610\n",
    "clean_tac_data = pd.read_csv('../data/clean_tac/JB3156_clean_TAC.csv')\n",
    "# clean_tac_data = pd.read_csv('../data/clean_tac/JB3156_clean_TAC.csv')\n",
    "clean_tac_data[\"tac\"] = np.where(clean_tac_data[\"TAC_Reading\"] > 0.08, 1, 0)\n",
    "clean_tac_data = clean_tac_data.drop(columns=\"TAC_Reading\")\n",
    "clean_tac_data = clean_tac_data.rename(columns={\"tac\": \"TAC_Reading\"})\n",
    "# clean_tac_data.describe()\n",
    "\n",
    "\n",
    "# Filtering for specific PID (temps)\n",
    "acc_data_pid = acc_data[acc_data.pid == \"JB3156\"]\n",
    "# acc_data_pid['pid'].unique()\n",
    "\n",
    "# Up sampling tac data to match acc data\n",
    "clean_ts = clean_tac_data['timestamp'] \n",
    "acc_ts = acc_data_pid['time']\n",
    "all_labels = list()\n",
    "offset_tac, offset_acc = 0, 0\n",
    "# print(acc_ts.iloc[0])\n",
    "# print(clean_ts.loc[0])\n",
    "# print(clean_tac_data.loc[0]['TAC_Reading'])\n",
    "# # acc_ts.iloc[0] #1493735870653\n",
    "while offset_tac < len(clean_ts) and offset_acc < len(acc_ts):\n",
    "  \n",
    "  while acc_ts.iloc[offset_acc] < clean_ts.iloc[offset_tac]:\n",
    "    all_labels.append([clean_tac_data.iloc[offset_tac]['TAC_Reading'], acc_ts.iloc[offset_acc]])\n",
    "    offset_acc += 1\n",
    "    if offset_acc >= len(acc_ts):\n",
    "      break\n",
    "\n",
    "  offset_tac += 1\n",
    "\n",
    "# all_labels\n",
    "\n",
    "all_labels_df = pd.DataFrame(all_labels, columns = [\"tac\", \"time\"])\n",
    "# all_labels_df.shape, acc_data_pid.shape\n",
    "\n",
    "# acc_data_pid['tac_reading'] = \n",
    "# TODO: Make sure tac data is sorted on timestamp\n",
    "clean_tac_data[\"from\"] = clean_tac_data[\"timestamp\"].shift(1, fill_value=-1) + 1\n",
    "\n",
    "clean_tac_data.index = pd.IntervalIndex.from_arrays(clean_tac_data[\"from\"], clean_tac_data[\"timestamp\"], closed = \"both\")\n",
    "acc_data_pid['tac'] = acc_data_pid[\"time\"].apply(lambda x: clean_tac_data.iloc[clean_tac_data.index.get_loc(x)][\"TAC_Reading\"])\n",
    "# acc_data_pid\n",
    "\n",
    "# TODO: Make n = 10 after either removing one record which has 7 records for a second or by adding 3 dummy values to it (latter is better)\n",
    "# frame_temp.groupby([ \"pid\", \"window10\"]).count().describe()\n",
    "# We are sampling with replacement, which should be okay since it is within a second\n",
    "acc_data_pid_20s = acc_data_pid.groupby([ \"pid\", \"time\"]).sample(n = 20, replace=True)\n",
    "\n",
    "acc_data_sliding = acc_data_pid_20s.copy()\n",
    "\n",
    "# # x_sliding_window.shape\n",
    "pids = [\"JB3156\"]\n",
    "final = []\n",
    "labels = []\n",
    "for pid in pids:\n",
    "  # temptemp = acc_data_pid_20s[acc_data_pid_20s['pid'] == pid]\n",
    "  temptemp = acc_data_sliding[acc_data_sliding['pid'] == pid]\n",
    "  times = temptemp.time.unique()\n",
    "  final_temp =[]\n",
    "  labels_temp = []\n",
    "  for i in range(len(times)):\n",
    "    # x = np.lib.stride_tricks.sliding_window_view(frame_temp2[frame_temp2.pid == pid and frame_temp2.window10 == time], window_shape = 10)\n",
    "    # temptemptemp = temptemp[temptemp['time'] == time]\n",
    "    time_to_filter = [times[j] if j >= 0 else -1 for j in range(i-9, i+1)]\n",
    "    # print(time_to_filter)\n",
    "    # if i == 10:\n",
    "    #   break\n",
    "    temptemptemp = temptemp[temptemp['time'].isin(time_to_filter)]\n",
    "    # TODO: Create x y z sliding windows\n",
    "    # x = np.lib.stride_tricks.sliding_window_view(temptemptemp[\"x\"], window_shape = 10)\n",
    "    # y = np.lib.stride_tricks.sliding_window_view(temptemptemp[\"y\"], window_shape = 10)\n",
    "    # z = np.lib.stride_tricks.sliding_window_view(temptemptemp[\"z\"], window_shape = 10)\n",
    "\n",
    "    # x_dash = np.array(temptemptemp[[\"x_\"+str(i) for i in range(window_size)]]).flatten()\n",
    "    # y_dash = np.array(temptemptemp[[\"y_\"+str(i) for i in range(window_size)]]).flatten()\n",
    "    # z_dash = np.array(temptemptemp[[\"z_\"+str(i) for i in range(window_size)]]).flatten()\n",
    "\n",
    "    x_dash = np.array(temptemptemp[\"x\"])\n",
    "    y_dash = np.array(temptemptemp[\"y\"])\n",
    "    z_dash = np.array(temptemptemp[\"z\"])\n",
    "\n",
    "    x_dash = np.pad(x_dash, (200 - len(x_dash), 0), \"constant\")\n",
    "    y_dash = np.pad(y_dash, (200 - len(y_dash), 0), \"constant\")\n",
    "    z_dash = np.pad(z_dash, (200 - len(z_dash), 0), \"constant\")\n",
    "\n",
    "    # a = np.vstack((temptemptemp[\"x\"].apply(lambda x: np.array(x, dtype=\"float32\")), temptemptemp[\"y\"].apply(lambda x: np.array(x, dtype=\"float32\")), temptemptemp[\"z\"].apply(lambda x: np.array(x, dtype=\"float32\"))))\n",
    "    a = np.transpose(np.vstack((x_dash, y_dash, z_dash)))\n",
    "    final_temp.append(a)\n",
    "    labels_temp.append(temptemptemp.head(1)[\"tac\"])\n",
    "  final.append(np.array(final_temp))\n",
    "  labels.append(np.array(labels_temp))\n",
    "  # print(final)\n",
    "  \n",
    "  # break\n",
    "# print(np.array(final,dtype=object).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 25559, 200, 3)\n",
      "(1, 25559, 1)\n"
     ]
    }
   ],
   "source": [
    "final_arr = np.asarray(final).astype('float32')\n",
    "\n",
    "# final_arr = np.reshape(final_arr, (final_arr.shape[0], final_arr.shape[1], final_arr.shape[3], final_arr.shape[2]))\n",
    "\n",
    "labels_arr = np.asarray(labels).astype('float32')\n",
    "print(final_arr.shape)\n",
    "print(labels_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25559, 200, 3), (25559, 1))"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_arr_reshape = np.reshape(final_arr, (25559, 200, 3)) \n",
    "labels_arr_reshape = np.reshape(labels_arr, (25559,1))\n",
    "final_arr_reshape.shape, labels_arr_reshape.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3410338560.0, 0.5765483975410461)"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, accuracy = base_model.evaluate(final_arr_reshape, labels_arr_reshape,\n",
    "                                    #  batch_size = batch_size, \n",
    "                                     verbose=0)\n",
    "\n",
    "round(loss,4), accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# class MyModel(tf.keras.Model):\n",
    "\n",
    "#   def __init__(self):\n",
    "#     super().__init__()\n",
    "#     flatten = tf.keras.layers.Flatten()\n",
    "#     conv_layer1 = tf.keras.layers.Conv1D(filters = 64, kernel_size = 3)\n",
    "#     conv_layer2 = tf.keras.layers.Conv1D(filters = 64, kernel_size = 3)\n",
    "#     dropout = tf.keras.layers.Dropout(0.5)\n",
    "#     max_pooling = tf.keras.layers.MaxPool1D(pool_size=2)\n",
    "#     # fc - fully connected layer\n",
    "#     fc_layer = tf.keras.layers.Dense(units=64, activation = 'relu')\n",
    "#     fc_layer2 = tf.keras.layers.Dense(units=1, activation = 'softmax') \n",
    "#     self.base_model = tf.keras.Sequential([\n",
    "#                                   conv_layer1, \n",
    "#                                   conv_layer2, \n",
    "#                                   dropout, \n",
    "#                                   max_pooling, \n",
    "#                                   # flatten, \n",
    "#                                   fc_layer, \n",
    "#                                   fc_layer2\n",
    "#                                 ])\n",
    "\n",
    "\n",
    "#   def call(self, inputs, training=True):\n",
    "#     return self.base_model(inputs)\n",
    "    \n",
    "#     # if training:\n",
    "#     #   x = self.dropout(x, training=training)\n",
    "#     # return self.dense2(x)\n",
    "\n",
    "# model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), \\\n",
    "#                    optimizer=tf.keras.optimizers.Adam(0.001), \\\n",
    "#                    metrics=[tf.keras.metrics.Accuracy()]) \n",
    "\n",
    "# model.build((30735, 3, 200))\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(final_arr_reshape, labels_arr_reshape, epochs = 100,\n",
    "#             #    batch_size = batch_size, \n",
    "#                verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 50\n",
    "# prep_data = pd.read_csv('../data/good_again_bhas.csv')\n",
    "flatten = tf.keras.layers.Flatten()\n",
    "conv_layer1 = tf.keras.layers.Conv1D(filters = 64, kernel_size = 3, padding='SAME')\n",
    "conv_layer2 = tf.keras.layers.Conv1D(filters = 64, kernel_size = 3, padding='SAME')\n",
    "dropout = tf.keras.layers.Dropout(0.5)\n",
    "max_pooling = tf.keras.layers.MaxPool1D(pool_size=2)\n",
    "# fc - fully connected layer\n",
    "fc_layer = tf.keras.layers.Dense(units=128, activation = 'relu')\n",
    "fc_layer2 = tf.keras.layers.Dense(units=1, activation = 'softmax')\n",
    "base_model = tf.keras.Sequential([\n",
    "                                  conv_layer1,  \n",
    "                                  conv_layer2, \n",
    "                                  dropout, \n",
    "                                  max_pooling, \n",
    "                                  flatten, \n",
    "                                  fc_layer, \n",
    "                                  fc_layer2\n",
    "                                ])\n",
    "\n",
    "# base_model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False, label_smoothing=0.0001), \\\n",
    "base_model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), \\\n",
    "                   optimizer=tf.keras.optimizers.Adam(0.001), \\\n",
    "                   metrics=[tf.keras.metrics.Accuracy()]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_func = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "# optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "\n",
    "# for e in range(0, 5000):\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         preds = base_model(final_arr_reshape)\n",
    "#         print(preds)\n",
    "        \n",
    "#         # loss = loss_func(y_true=labels_arr_reshape, y_pred=preds)\n",
    "#         # acc = np.sum(np.equal(labels, preds)) / 30735\n",
    "#     #     print(acc)\n",
    "#     # gradients = tape.gradient(loss, base_model.trainable_variables)\n",
    "#     # optimizer.apply_gradients(zip(gradients, base_model.trainable_variables))\n",
    "#     # print(f\"Epoch: {e} | LOSS : {loss} | acc {acc}\")\n",
    "\n",
    "# # loss\n",
    "            \n",
    "# # total_loss += loss\n",
    "# # gradients = tape.gradient(loss, model.trainable_variables)\n",
    "# #     optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "# #     return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.build((30735, 200, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_57\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_48 (Conv1D)          (30735, 200, 64)          640       \n",
      "                                                                 \n",
      " conv1d_49 (Conv1D)          (30735, 200, 64)          12352     \n",
      "                                                                 \n",
      " dropout_31 (Dropout)        (30735, 200, 64)          0         \n",
      "                                                                 \n",
      " max_pooling1d_24 (MaxPoolin  (30735, 100, 64)         0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten_55 (Flatten)        (30735, 6400)             0         \n",
      "                                                                 \n",
      " dense_140 (Dense)           (30735, 128)              819328    \n",
      "                                                                 \n",
      " dense_141 (Dense)           (30735, 1)                129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 832,449\n",
      "Trainable params: 832,449\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 1.], dtype=float32), array([11238, 19497]))"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(labels_arr, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1225727, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tac\n",
       "1    778034\n",
       "0    447693\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(all_labels_df.shape)\n",
    "all_labels_df['tac'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "961/961 [==============================] - 7s 7ms/step - loss: 0.6206 - accuracy: 0.6344\n",
      "Epoch 2/50\n",
      "961/961 [==============================] - 7s 7ms/step - loss: 0.5941 - accuracy: 0.6344\n",
      "Epoch 3/50\n",
      "961/961 [==============================] - 7s 7ms/step - loss: 0.5795 - accuracy: 0.6344\n",
      "Epoch 4/50\n",
      "136/961 [===>..........................] - ETA: 5s - loss: 0.5739 - accuracy: 0.6321"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[394], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m base_model\u001b[39m.\u001b[39;49mfit(final_arr_reshape, labels_arr_reshape, epochs \u001b[39m=\u001b[39;49m epochs,\n\u001b[1;32m      2\u001b[0m                batch_size \u001b[39m=\u001b[39;49m batch_size, \n\u001b[1;32m      3\u001b[0m                verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m) \n\u001b[1;32m      5\u001b[0m \u001b[39m# base_model.fit(base_model_data, base_model_labels, epochs = 100,\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m#             #    batch_size = batch_size, \u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39m#                verbose=1) \u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.10/site-packages/keras/engine/training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1642\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1643\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1644\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1648\u001b[0m ):\n\u001b[1;32m   1649\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1650\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1651\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1652\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    909\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    910\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    911\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 912\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    915\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    916\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m   (concrete_function,\n\u001b[1;32m    133\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    135\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1741\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1742\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1743\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1744\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1746\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1747\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m     args,\n\u001b[1;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1750\u001b[0m     executing_eagerly)\n\u001b[1;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    377\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    379\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    380\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    381\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    382\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    383\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    384\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    386\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    387\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    391\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "base_model.fit(final_arr_reshape, labels_arr_reshape, epochs = epochs,\n",
    "               batch_size = batch_size, \n",
    "               verbose=1) \n",
    "\n",
    "# base_model.fit(base_model_data, base_model_labels, epochs = 100,\n",
    "#             #    batch_size = batch_size, \n",
    "#                verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5554, 0.6346510648727417)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss, accuracy = base_model.evaluate(final_arr_reshape, labels_arr_reshape,\n",
    "#                                     #  batch_size = batch_size, \n",
    "#                                      verbose=0)\n",
    "\n",
    "loss, accuracy = base_model.evaluate(base_model_data, base_model_labels,\n",
    "                                    #  batch_size = batch_size, \n",
    "                                     verbose=0)\n",
    "\n",
    "round(loss,4), accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "clf = MLPClassifier(solver='adam', shuffle=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarah_prakriti_peters/miniconda3/envs/DL/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1098: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/sarah_prakriti_peters/miniconda3/envs/DL/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'activation': 'relu',\n",
       " 'alpha': 0.0001,\n",
       " 'batch_size': 'auto',\n",
       " 'beta_1': 0.9,\n",
       " 'beta_2': 0.999,\n",
       " 'early_stopping': False,\n",
       " 'epsilon': 1e-08,\n",
       " 'hidden_layer_sizes': (100,),\n",
       " 'learning_rate': 'constant',\n",
       " 'learning_rate_init': 0.001,\n",
       " 'max_fun': 15000,\n",
       " 'max_iter': 200,\n",
       " 'momentum': 0.9,\n",
       " 'n_iter_no_change': 10,\n",
       " 'nesterovs_momentum': True,\n",
       " 'power_t': 0.5,\n",
       " 'random_state': 1,\n",
       " 'shuffle': True,\n",
       " 'solver': 'adam',\n",
       " 'tol': 0.0001,\n",
       " 'validation_fraction': 0.1,\n",
       " 'verbose': False,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_arr_mlp = np.reshape(final_arr, (30735, 600))\n",
    "labels_arr_mlp = np.reshape(labels_arr, (30735,1))\n",
    "\n",
    "clf.fit(final_arr_mlp, labels_arr_mlp)\n",
    "clf.get_params()\n",
    "\n",
    "# print('Accuracy ', accuracy_score(y_test, clf.predict(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy  0.9113714006832602\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy ', accuracy_score(labels_arr_mlp, clf.predict(final_arr_mlp)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model \n",
    "#### LSTM Layer (128 units), Dropout layer (p=0.5), Dense layer (128 units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_29\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_6 (LSTM)               (30735, 128)              67584     \n",
      "                                                                 \n",
      " dropout_23 (Dropout)        (30735, 128)              0         \n",
      "                                                                 \n",
      " dense_66 (Dense)            (30735, 128)              16512     \n",
      "                                                                 \n",
      " dense_67 (Dense)            (30735, 1)                129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 84,225\n",
      "Trainable params: 84,225\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model_data = np.reshape(final_arr, (30735, 200, 3)) \n",
    "base_model_labels = np.reshape(labels_arr, (30735,1))\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "# prep_data = pd.read_csv('../data/good_again_bhas.csv')\n",
    "flatten = tf.keras.layers.Flatten()\n",
    "lstm_layer = tf.keras.layers.LSTM(units=128)\n",
    "dropout_layer = tf.keras.layers.Dropout(0.5)\n",
    "fc_layer = tf.keras.layers.Dense(units=128, activation='relu', kernel_regularizer = 'l2')\n",
    "fc_layer2 = tf.keras.layers.Dense(units=1, activation='softmax')\n",
    "lstm_model = tf.keras.Sequential([lstm_layer, dropout_layer,\n",
    "                                # flatten, \n",
    "                                fc_layer, fc_layer2])\n",
    "\n",
    "lstm_model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), \\\n",
    "                   optimizer=tf.keras.optimizers.Adam(0.03), \\\n",
    "                   metrics=[tf.keras.metrics.Accuracy()]) \n",
    "\n",
    "lstm_model.build((30735, 200, 3))\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "961/961 [==============================] - 85s 86ms/step - loss: 0.6732 - accuracy: 0.6347\n",
      "Epoch 2/100\n",
      "961/961 [==============================] - 81s 85ms/step - loss: 0.6574 - accuracy: 0.6347\n",
      "Epoch 3/100\n",
      "961/961 [==============================] - 81s 84ms/step - loss: 0.6569 - accuracy: 0.6347\n",
      "Epoch 4/100\n",
      "961/961 [==============================] - 82s 85ms/step - loss: 0.6571 - accuracy: 0.6347\n",
      "Epoch 5/100\n",
      "961/961 [==============================] - 82s 85ms/step - loss: 0.6571 - accuracy: 0.6347\n",
      "Epoch 6/100\n",
      "961/961 [==============================] - 83s 86ms/step - loss: 0.6571 - accuracy: 0.6347\n",
      "Epoch 7/100\n",
      "266/961 [=======>......................] - ETA: 1:00 - loss: 0.6546 - accuracy: 0.6397"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[196], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m lstm_model\u001b[39m.\u001b[39;49mfit(base_model_data, base_model_labels, epochs \u001b[39m=\u001b[39;49m \u001b[39m100\u001b[39;49m,\n\u001b[1;32m      2\u001b[0m             \u001b[39m#    batch_size = batch_size, \u001b[39;49;00m\n\u001b[1;32m      3\u001b[0m                verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m) \n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.10/site-packages/keras/engine/training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1642\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1643\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1644\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1648\u001b[0m ):\n\u001b[1;32m   1649\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1650\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1651\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1652\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    909\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    910\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    911\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 912\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    915\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    916\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m   (concrete_function,\n\u001b[1;32m    133\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    135\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1741\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1742\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1743\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1744\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1746\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1747\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m     args,\n\u001b[1;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1750\u001b[0m     executing_eagerly)\n\u001b[1;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    377\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    379\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    380\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    381\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    382\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    383\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    384\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    386\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    387\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    391\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad pipe message: %s [b'\\xb9\\xe3\\xb2\\x16\\x96\\xe7\\xae\\xcd\\xd3\\x02\\x08M\\xd6\\xdc\\xa5u\\xeb\\xfd \\x11\\t\\xdc@ \\xa9>\\xb3\\xbb\\x88\\x18\\x94\\xca!B\\xab\\xbeaS\\xb6\\x88;\\x00\\xf5\\x15\\xc2\\x13A\\xd5M3@\\x00 ::\\x13\\x01\\x13\\x02\\x13\\x03\\xc0+\\xc0/\\xc0,\\xc00\\xcc\\xa9\\xcc\\xa8\\xc0\\x13\\xc0\\x14\\x00\\x9c\\x00\\x9d\\x00/\\x005\\x01\\x00\\x01\\x93\\xea\\xea\\x00\\x00\\x00\\x0b\\x00\\x02\\x01\\x00\\x00\\x17\\x00\\x00\\x00\\n\\x00\\n\\x00\\x08zz\\x00\\x1d\\x00\\x17\\x00\\x18\\x003\\x00+\\x00)zz\\x00\\x01\\x00\\x00\\x1d\\x00 lQ\\x8a\\x99\\x04\\xaf\\xc1\\x96\\x83\\x0f\\xa5,(\\xde1\\x18\\x9d\\xb1JEZ\"P\\xf5\\xba\\xf7\\xfdQy\\xe6{W\\x00\\x10\\x00\\x0e\\x00\\x0c\\x02h2\\x08http/1.1\\x00\\x12\\x00\\x00\\xff\\x01\\x00\\x01\\x00\\x00\\x05\\x00\\x05\\x01\\x00\\x00\\x00\\x00\\x00\\x1b\\x00\\x03\\x02\\x00\\x02\\x00-\\x00\\x02\\x01\\x01\\x00+\\x00\\x07\\x06jj\\x03\\x04\\x03\\x03\\x00#\\x00\\x00Di\\x00\\x05\\x00\\x03\\x02']\n",
      "Bad pipe message: %s [b'\\xfa}\\xe5\\xe9v\\r\\xc4a\\x8c\\xa0\\x9d\\x8d\\xb6a\\xb1$\\xbcG \\x12\\x9c2\\xab\\xb0\\xe0\\xbd3\\x00\\x8d\\xab\\xc5\\x86\\x81b.<\\xbb\\xaa\\xb6\\xd7\\xb6\\xc7\\x16+\\xb6f\\n\\x9d\\x91a\\xe4\\x00 zz\\x13\\x01\\x13\\x02\\x13\\x03\\xc0+\\xc0/\\xc0,\\xc00\\xcc\\xa9\\xcc\\xa8\\xc0\\x13\\xc0\\x14\\x00\\x9c\\x00\\x9d\\x00/\\x005\\x01\\x00\\x01\\x93\\xaa\\xaa\\x00\\x00\\x00\\x05\\x00\\x05\\x01\\x00\\x00\\x00\\x00\\x00#\\x00\\x00\\x00\\n\\x00\\n\\x00\\x08\\n\\n\\x00\\x1d\\x00\\x17\\x00', b'\\x01\\x00\\x01\\x00\\x003\\x00+\\x00)\\n\\n\\x00\\x01\\x00\\x00\\x1d\\x00 \\xb6\\xd4\\xbf\\xb5', b'\\xc1\\xe2Y\\xd6\\xfem\\xeam|f\\x93\\xe1:R\\x92\\xac\\x1a\\xc9\\x9b}jqw\\xf5\\xc2P\\x00+\\x00\\x07\\x06JJ\\x03\\x04\\x03\\x03\\x00-\\x00\\x02\\x01\\x01\\x00\\x0b\\x00\\x02\\x01\\x00\\x00\\x10\\x00\\x0e\\x00\\x0c\\x02h2\\x08http/1.1\\x00\\r\\x00\\x14\\x00\\x12\\x04\\x03\\x08\\x04\\x04\\x01\\x05\\x03\\x08\\x05\\x05\\x01\\x08\\x06\\x06\\x01\\x02\\x01\\x00\\x17\\x00\\x00\\x00\\x12\\x00\\x00Di\\x00\\x05\\x00\\x03\\x02h2\\x00\\x1b\\x00\\x03\\x02\\x00\\x02**\\x00\\x01\\x00\\x00\\x15\\x00\\xde']\n",
      "Bad pipe message: %s [b'\\x00\\r\\x00\\x12\\x00\\x10\\x04\\x03\\x08\\x04\\x04\\x01\\x05\\x03\\x08\\x05\\x05\\x01\\x08\\x06\\x06\\x01zz\\x00\\x01\\x00\\x00\\x15\\x00\\xe0\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00']\n"
     ]
    }
   ],
   "source": [
    "lstm_model.fit(base_model_data, base_model_labels, epochs = 100,\n",
    "            #    batch_size = batch_size, \n",
    "               verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "961/961 [==============================] - 2s 1ms/step - loss: 0.5258 - accuracy: 0.6347\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5258, 0.6346510648727417)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad pipe message: %s [b'\\x02P\\xcep\\xd4\\x9d\\xebA\\x8a7+\\x9a+\\xc0\\xbe\\x08\\xa8\\x9d 2\\xd4\\xb9\\xf5;\\xc4\\xe0\\xde\\xdeeKk\\x1e\\x01\\x8e\\xc3\\x86\\xb5\\x03\\x0e\\x8e\\x83\\x82@\\xfe\\x06\\x863Z\\x91V\\x87\\x00 **\\x13\\x01\\x13\\x02\\x13\\x03\\xc0+\\xc0/\\xc0,\\xc00\\xcc\\xa9\\xcc\\xa8\\xc0\\x13\\xc0\\x14\\x00\\x9c\\x00\\x9d\\x00/\\x005\\x01\\x00\\x01\\x93\\xba\\xba\\x00\\x00\\x00\\x05\\x00\\x05\\x01\\x00\\x00\\x00\\x00\\x00\\x1b\\x00\\x03\\x02\\x00\\x02\\x00\\x17\\x00\\x00\\x00\\n\\x00\\n\\x00\\x08\\x8a\\x8a\\x00\\x1d\\x00\\x17\\x00\\x18\\xff\\x01\\x00\\x01\\x00Di\\x00\\x05\\x00\\x03\\x02h2\\x00\\r\\x00\\x12\\x00\\x10\\x04\\x03\\x08\\x04\\x04\\x01\\x05\\x03\\x08\\x05\\x05\\x01\\x08\\x06\\x06\\x01\\x003\\x00+\\x00)\\x8a\\x8a\\x00\\x01\\x00\\x00\\x1d\\x00 \\x15\\xbb\\xcc+\\xdf\\xd6R\\xc2\\xf4i\\x96\\xd2{x^\\xe4#\\xd4>\\x9cq\\xb6[']\n",
      "Bad pipe message: %s [b'\\x8e\\x16\\xea\\xe5\\x02\\x89\\x08\\x00+\\x00\\x07\\x06\\xba\\xba\\x03\\x04\\x03\\x03\\x00\\x0b\\x00\\x02\\x01\\x00\\x00\\x10\\x00\\x0e\\x00\\x0c\\x02h2\\x08http/1.1\\x00-\\x00\\x02\\x01\\x01\\x00\\x12\\x00\\x00\\x00#\\x00\\x00\\xfa\\xfa\\x00\\x01\\x00\\x00\\x15\\x00\\xe0\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00']\n",
      "Bad pipe message: %s [b'R\\xdd\\xc8\\xca~u\\xa6>`\\x0fD;\\xfe\\x81\\x92\\x1ag( \\xf3\\xed\\xdf\\xda4\\x8d\\x08\\x17nE\\xbf\\x82w\\x9e\\x9b?z2\\x9d\\xe3\\x96\\xcc\\x1a(!\\xe3j\\x11U\\x95\\xd7\\x9b\\x00 jj\\x13\\x01\\x13\\x02\\x13\\x03\\xc0+\\xc0/\\xc0,\\xc00\\xcc\\xa9\\xcc\\xa8\\xc0\\x13\\xc0\\x14\\x00\\x9c\\x00\\x9d\\x00/\\x005\\x01\\x00\\x01\\x93\\xaa\\xaa\\x00\\x00\\xff\\x01\\x00\\x01\\x00\\x00\\x05\\x00\\x05\\x01\\x00\\x00\\x00\\x00Di\\x00\\x05\\x00\\x03\\x02h2\\x00-\\x00\\x02\\x01\\x01\\x00+\\x00\\x07\\x06\\xca\\xca\\x03\\x04\\x03\\x03\\x003\\x00+\\x00)jj\\x00\\x01\\x00\\x00\\x1d\\x00 \\xbf\\xec\\x06T\\x12\\xfa\\xd5\\x7f8z\\xdb\\x08D\"\\xfe\\xeaX\\xb6G\\x1e\\x1c\\xc5.\\xb8\\xee\\xe7m<&ova\\x00\\n\\x00\\n\\x00\\x08jj\\x00']\n",
      "Bad pipe message: %s [b'\\x17\\x00\\x18\\x00\\x10\\x00\\x0e\\x00\\x0c\\x02h2\\x08http/1.1\\x00\\x1b\\x00\\x03\\x02\\x00\\x02']\n",
      "Bad pipe message: %s [b'\\xcfdP<\\xe2\\xa0m\\x80\\xe7\\xe7~m\\xf6\\xdaI\\xe9\\xde\\xda \\x07I\\xaf\\x95\\x05\\xf8\\x1f\\xa3%', b'b2\\x9a\\x9ei\\xb0a\\x034\\xf4p\\x99\\xefmS\\\\\\x1eh\\x1e', b'\\x00 \\x1a\\x1a\\x13\\x01\\x13\\x02\\x13\\x03\\xc0+\\xc0/\\xc0,\\xc00\\xcc\\xa9\\xcc\\xa8\\xc0\\x13\\xc0\\x14\\x00\\x9c\\x00\\x9d\\x00/\\x005\\x01\\x00\\x01\\x93\\x8a\\x8a\\x00\\x00\\x00\\x0b\\x00\\x02\\x01\\x00\\x00\\x05\\x00\\x05\\x01\\x00\\x00\\x00\\x00\\x003\\x00+\\x00)\\xba\\xba\\x00\\x01\\x00\\x00\\x1d\\x00 \\x9dP\\xc3U\\tm\\xa0\\xc2\\x9b\\x9aS\\x01W6\\xbb\\x042\\xbb\\x17|\\xb4b\\x8d\\xefqh\\x14l<\\x15\\xb9\\x00\\x00\\x12\\x00\\x00\\x00\\n\\x00\\n\\x00\\x08\\xba\\xba\\x00\\x1d\\x00\\x17\\x00\\x18\\x00\\x1b\\x00\\x03\\x02\\x00\\x02\\x00-\\x00\\x02\\x01\\x01\\x00#\\x00\\x00']\n",
      "Bad pipe message: %s [b'r\\xbbx\\xae^U\\xf2o\\xee;\\xd7\\xcb\\xa5\\xce\\xcd\\x01\\x85\\xdf :\\x84<\\x0c\\xe0!//)\\x18\\xc0m\\xd9\\x14~\\xc3\\x14\\xe5\\xb0\\xd4[R\\xb8\\x17\\xf5\\xdb\\x88\\x98\\x88+;\\x17\\x00 \\x8a\\x8a\\x13\\x01\\x13\\x02\\x13\\x03\\xc0+\\xc0/\\xc0,\\xc00\\xcc\\xa9\\xcc\\xa8\\xc0\\x13\\xc0\\x14\\x00\\x9c\\x00\\x9d\\x00/\\x005\\x01\\x00\\x01\\x93zz\\x00\\x00\\x00#\\x00\\x00\\x00\\n\\x00\\n\\x00\\x08**\\x00\\x1d\\x00\\x17\\x00\\x18\\x00\\x05\\x00\\x05\\x01\\x00\\x00\\x00\\x00\\xff\\x01\\x00\\x01\\x00\\x00+\\x00\\x07\\x06\\xaa\\xaa\\x03\\x04\\x03\\x03']\n",
      "Bad pipe message: %s [b\"\\xf8N\\xac\\xf8\\xe9}\\x86'\\x9fd\\xb4\\xa6<\\xee\\xa7!\\xc2\\xaa \\xad\\xd1\\tV@\\xddu\\xd5R\\xb1\\x9e\\x19\\x19\\xac,\\xf6\\xca\\x16\\x8eOp\\xc7\\x83/\\xef\\x02\\x0c\\x17\\xcf0\\x1f\\xb9\\x00 \\xaa\\xaa\\x13\\x01\\x13\\x02\\x13\\x03\\xc0+\\xc0/\\xc0,\\xc00\\xcc\\xa9\\xcc\\xa8\\xc0\\x13\\xc0\\x14\\x00\\x9c\\x00\\x9d\\x00/\\x005\\x01\\x00\\x01\\x93ZZ\\x00\\x00\\x00\\x1b\\x00\\x03\\x02\\x00\\x02\\x00\\x12\\x00\\x00\\x00#\\x00\\x00\\x00\\x0b\\x00\\x02\\x01\\x00\\x00\\x10\\x00\\x0e\\x00\\x0c\\x02h2\\x08http\", b'.1\\x003\\x00+\\x00)JJ\\x00\\x01\\x00\\x00\\x1d\\x00 \\xc5\\x91\\xf5#Ls\\x8a\\xaasRW5\\xeeN\\xf6\\xadY35f\\xf2\\xce*\\x18\\xac2A\\x83\\x00', b'q\\x00\\n']\n",
      "Bad pipe message: %s [b\"o\\x8b'XF\\x82\\x02\\xae\\xc3\\x17\\xea\\xa6\\xb6\\x84\"]\n",
      "Bad pipe message: %s [b\"-\\x1e \\xbf\\x84\\xf1a\\xd0\\xd6\\xee\\x81#q\\x81*\\x89u\\xea\\xc5Pt\\x07I\\xd6\\x01\\xb0n\\xf3o'\\xf0h\\x18\\xfb\\x11\\x00 \\x1a\\x1a\\x13\\x01\\x13\\x02\\x13\\x03\\xc0+\\xc0/\\xc0,\\xc00\\xcc\\xa9\\xcc\\xa8\\xc0\\x13\\xc0\\x14\\x00\\x9c\\x00\\x9d\\x00/\\x005\\x01\\x00\\x01\\x93\\xba\\xba\\x00\\x00\\x00\\r\\x00\\x14\\x00\\x12\\x04\\x03\\x08\\x04\\x04\\x01\\x05\\x03\\x08\\x05\\x05\\x01\\x08\\x06\\x06\\x01\\x02\\x01\\x00\\n\\x00\\n\\x00\\x08\\xda\\xda\\x00\\x1d\\x00\\x17\\x00\\x18Di\\x00\", b'\\x03\\x02h2']\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = lstm_model.evaluate(base_model_data, base_model_labels,\n",
    "                                    #  batch_size = batch_size, \n",
    "                                     verbose=1)\n",
    "\n",
    "round(loss,4),accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('csci1470')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d9ae1d7aab292b742cce88b46a7d39d3e8f6dede2c2111bf10ed84b03dba6379"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
