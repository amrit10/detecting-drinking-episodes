{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import scipy\n",
    "from scipy.stats import skew, kurtosis\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JB3156</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CC6740</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SA0297</td>\n",
       "      <td>0.0758</td>\n",
       "      <td>0.0273</td>\n",
       "      <td>-0.0102</td>\n",
       "      <td>1493733882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SA0297</td>\n",
       "      <td>-0.0359</td>\n",
       "      <td>0.0794</td>\n",
       "      <td>0.0037</td>\n",
       "      <td>1493733882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SA0297</td>\n",
       "      <td>-0.2427</td>\n",
       "      <td>-0.0861</td>\n",
       "      <td>-0.0163</td>\n",
       "      <td>1493733882</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      pid       x       y       z        time\n",
       "0  JB3156  0.0000  0.0000  0.0000           0\n",
       "1  CC6740  0.0000  0.0000  0.0000           0\n",
       "2  SA0297  0.0758  0.0273 -0.0102  1493733882\n",
       "3  SA0297 -0.0359  0.0794  0.0037  1493733882\n",
       "4  SA0297 -0.2427 -0.0861 -0.0163  1493733882"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read Accelerometer Data\n",
    "acc_data = pd.read_csv('../data/all_accelerometer_data_pids_13.csv')\n",
    "\n",
    "\n",
    "def get_time_value(x):\n",
    "  # x is ms. it is divided by 1000 to get microsecond\n",
    "  t = datetime.datetime.fromtimestamp(x/1000.0)\n",
    "  t = t.replace(microsecond = 0)\n",
    "  return int(t.timestamp())\n",
    "\n",
    "acc_data['window10'] = acc_data['time'].apply(get_time_value)\n",
    "acc_data = acc_data.drop(columns=\"time\")\n",
    "acc_data = acc_data.rename(columns = {\"window10\": \"time\"})\n",
    "\n",
    "acc_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['JB3156', 'CC6740', 'SA0297', 'PC6771', 'BK7610', 'DC6359',\n",
       "       'MC7070', 'MJ8002', 'BU4707', 'JR8022', 'HV0618', 'SF3079',\n",
       "       'DK3500'], dtype=object)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_data['pid'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>TAC_Reading</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.700000e+01</td>\n",
       "      <td>57.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.493758e+09</td>\n",
       "      <td>0.228070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.841595e+04</td>\n",
       "      <td>0.423318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.493719e+09</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.493729e+09</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.493756e+09</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.493782e+09</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.493808e+09</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          timestamp  TAC_Reading\n",
       "count  5.700000e+01    57.000000\n",
       "mean   1.493758e+09     0.228070\n",
       "std    2.841595e+04     0.423318\n",
       "min    1.493719e+09     0.000000\n",
       "25%    1.493729e+09     0.000000\n",
       "50%    1.493756e+09     0.000000\n",
       "75%    1.493782e+09     0.000000\n",
       "max    1.493808e+09     1.000000"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read clean tac data for pid = BK7610\n",
    "clean_tac_data = pd.read_csv('../data/clean_tac/BK7610_clean_TAC.csv')\n",
    "# clean_tac_data = pd.read_csv('../data/clean_tac/JB3156_clean_TAC.csv')\n",
    "clean_tac_data[\"tac\"] = np.where(clean_tac_data[\"TAC_Reading\"] > 0.08, 1, 0)\n",
    "clean_tac_data = clean_tac_data.drop(columns=\"TAC_Reading\")\n",
    "clean_tac_data = clean_tac_data.rename(columns={\"tac\": \"TAC_Reading\"})\n",
    "clean_tac_data.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BK7610'], dtype=object)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtering for specific PID (temps)\n",
    "acc_data_pid = acc_data[acc_data.pid == \"BK7610\"]\n",
    "acc_data_pid['pid'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.225727e+06</td>\n",
       "      <td>1.225727e+06</td>\n",
       "      <td>1.225727e+06</td>\n",
       "      <td>1.225727e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-6.497703e-03</td>\n",
       "      <td>7.507374e-03</td>\n",
       "      <td>2.747567e-03</td>\n",
       "      <td>1.493752e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.380473e-01</td>\n",
       "      <td>1.387602e-01</td>\n",
       "      <td>1.279124e-01</td>\n",
       "      <td>9.276766e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-4.274800e+00</td>\n",
       "      <td>-6.948900e+00</td>\n",
       "      <td>-5.277200e+00</td>\n",
       "      <td>1.493736e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-9.400000e-03</td>\n",
       "      <td>-6.000000e-03</td>\n",
       "      <td>-7.300000e-03</td>\n",
       "      <td>1.493744e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>5.700000e-03</td>\n",
       "      <td>1.493752e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.300000e-03</td>\n",
       "      <td>9.400000e-03</td>\n",
       "      <td>1.140000e-02</td>\n",
       "      <td>1.493760e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.450300e+00</td>\n",
       "      <td>5.344100e+00</td>\n",
       "      <td>4.656500e+00</td>\n",
       "      <td>1.493768e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  x             y             z          time\n",
       "count  1.225727e+06  1.225727e+06  1.225727e+06  1.225727e+06\n",
       "mean  -6.497703e-03  7.507374e-03  2.747567e-03  1.493752e+09\n",
       "std    1.380473e-01  1.387602e-01  1.279124e-01  9.276766e+03\n",
       "min   -4.274800e+00 -6.948900e+00 -5.277200e+00  1.493736e+09\n",
       "25%   -9.400000e-03 -6.000000e-03 -7.300000e-03  1.493744e+09\n",
       "50%    1.000000e-04  1.000000e-04  5.700000e-03  1.493752e+09\n",
       "75%    8.300000e-03  9.400000e-03  1.140000e-02  1.493760e+09\n",
       "max    6.450300e+00  5.344100e+00  4.656500e+00  1.493768e+09"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_data_pid.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1225727, 5)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_data_pid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1493735870],\n",
       " [0, 1493735870],\n",
       " [0, 1493735870],\n",
       " [0, 1493735870],\n",
       " [0, 1493735870],\n",
       " [0, 1493735870],\n",
       " [0, 1493735870],\n",
       " [0, 1493735870],\n",
       " [0, 1493735870],\n",
       " [0, 1493735870],\n",
       " [0, 1493735870],\n",
       " [0, 1493735870],\n",
       " [0, 1493735870],\n",
       " [0, 1493735870],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " ...]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Up sampling tac data to match acc data\n",
    "clean_ts = clean_tac_data['timestamp'] \n",
    "acc_ts = acc_data_pid['time']\n",
    "all_labels = list()\n",
    "offset_tac, offset_acc = 0, 0\n",
    "# print(acc_ts.iloc[0])\n",
    "# print(clean_ts.loc[0])\n",
    "# print(clean_tac_data.loc[0]['TAC_Reading'])\n",
    "# # acc_ts.iloc[0] #1493735870653\n",
    "while offset_tac < len(clean_ts) and offset_acc < len(acc_ts):\n",
    "  \n",
    "  while acc_ts.iloc[offset_acc] < clean_ts.iloc[offset_tac]:\n",
    "    all_labels.append([clean_tac_data.iloc[offset_tac]['TAC_Reading'], acc_ts.iloc[offset_acc]])\n",
    "    offset_acc += 1\n",
    "    if offset_acc >= len(acc_ts):\n",
    "      break\n",
    "\n",
    "  offset_tac += 1\n",
    "\n",
    "all_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1225727, 2), (1225727, 5))"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_labels_df = pd.DataFrame(all_labels, columns = [\"tac\", \"time\"])\n",
    "all_labels_df.shape, acc_data_pid.shape\n",
    "# merged = merged.drop_duplicates().reset_index(drop=True)\n",
    "# merged.to_csv(\"../data/BK7610_final_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged = acc_data_pid.head(10).merge(all_labels_df.head(10), on = 'time', how='inner')\n",
    "# merged['time'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(all_labels_df['time'].unique()), len(acc_data_pid['time'].unique())\n",
    "clean_tac_data[\"timestamp\"].is_monotonic_increasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_data_pid['tac_reading'] = \n",
    "# TODO: Make sure tac data is sorted on timestamp\n",
    "clean_tac_data[\"from\"] = clean_tac_data[\"timestamp\"].shift(1, fill_value=-1) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tac_data.index = pd.IntervalIndex.from_arrays(clean_tac_data[\"from\"], clean_tac_data[\"timestamp\"], closed = \"both\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7y/kyw1v_8j0g1ckfb3g6x93q1m0000gn/T/ipykernel_92662/2638206493.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  acc_data_pid['tac'] = acc_data_pid[\"time\"].apply(lambda x: clean_tac_data.iloc[clean_tac_data.index.get_loc(x)][\"TAC_Reading\"])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>time</th>\n",
       "      <th>tac</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47136</th>\n",
       "      <td>BK7610</td>\n",
       "      <td>0.1261</td>\n",
       "      <td>-0.0078</td>\n",
       "      <td>-0.0243</td>\n",
       "      <td>1493735870</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47138</th>\n",
       "      <td>BK7610</td>\n",
       "      <td>0.1336</td>\n",
       "      <td>-0.0697</td>\n",
       "      <td>-0.0446</td>\n",
       "      <td>1493735870</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47140</th>\n",
       "      <td>BK7610</td>\n",
       "      <td>0.1443</td>\n",
       "      <td>-0.0474</td>\n",
       "      <td>-0.0447</td>\n",
       "      <td>1493735870</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47142</th>\n",
       "      <td>BK7610</td>\n",
       "      <td>0.1255</td>\n",
       "      <td>-0.0038</td>\n",
       "      <td>0.0111</td>\n",
       "      <td>1493735870</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47144</th>\n",
       "      <td>BK7610</td>\n",
       "      <td>0.1076</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0276</td>\n",
       "      <td>1493735870</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6071104</th>\n",
       "      <td>BK7610</td>\n",
       "      <td>-0.0784</td>\n",
       "      <td>-0.0161</td>\n",
       "      <td>0.1719</td>\n",
       "      <td>1493767770</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6071108</th>\n",
       "      <td>BK7610</td>\n",
       "      <td>-0.0395</td>\n",
       "      <td>-0.0816</td>\n",
       "      <td>0.1634</td>\n",
       "      <td>1493767770</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6071112</th>\n",
       "      <td>BK7610</td>\n",
       "      <td>0.0160</td>\n",
       "      <td>-0.0853</td>\n",
       "      <td>0.0906</td>\n",
       "      <td>1493767770</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6071117</th>\n",
       "      <td>BK7610</td>\n",
       "      <td>0.0901</td>\n",
       "      <td>-0.0767</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>1493767770</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6071121</th>\n",
       "      <td>BK7610</td>\n",
       "      <td>0.1700</td>\n",
       "      <td>-0.1390</td>\n",
       "      <td>-0.0773</td>\n",
       "      <td>1493767770</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1225727 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            pid       x       y       z        time  tac\n",
       "47136    BK7610  0.1261 -0.0078 -0.0243  1493735870    0\n",
       "47138    BK7610  0.1336 -0.0697 -0.0446  1493735870    0\n",
       "47140    BK7610  0.1443 -0.0474 -0.0447  1493735870    0\n",
       "47142    BK7610  0.1255 -0.0038  0.0111  1493735870    0\n",
       "47144    BK7610  0.1076  0.0032  0.0276  1493735870    0\n",
       "...         ...     ...     ...     ...         ...  ...\n",
       "6071104  BK7610 -0.0784 -0.0161  0.1719  1493767770    1\n",
       "6071108  BK7610 -0.0395 -0.0816  0.1634  1493767770    1\n",
       "6071112  BK7610  0.0160 -0.0853  0.0906  1493767770    1\n",
       "6071117  BK7610  0.0901 -0.0767  0.0162  1493767770    1\n",
       "6071121  BK7610  0.1700 -0.1390 -0.0773  1493767770    1\n",
       "\n",
       "[1225727 rows x 6 columns]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_data_pid['tac'] = acc_data_pid[\"time\"].apply(lambda x: clean_tac_data.iloc[clean_tac_data.index.get_loc(x)][\"TAC_Reading\"])\n",
    "acc_data_pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47136, 6071121)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(acc_data_pid[\"tac\"].unique().sort())\n",
    "# clean_tac_data\n",
    "# min: 1,493,718,714\n",
    "# max: 1,493,807,899\n",
    "# \n",
    "# acc_data_pid\n",
    "# min: 1,493,735,870\n",
    "# max: 1,493,767,770\n",
    "# acc_data_pid[\"time\"].max()\n",
    "# groups = acc_data_pid.groupby([\"time\"])\n",
    "# print(groups.apply(lambda x: x[\"tac\"]<20))\n",
    "# time_stamps = groups.apply(lambda x: x[\"tac\"]<20)\n",
    "# time_stamps \n",
    "\n",
    "min(acc_data_pid.index), max(acc_data_pid.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make n = 10 after either removing one record which has 7 records for a second or by adding 3 dummy values to it (latter is better)\n",
    "# frame_temp.groupby([ \"pid\", \"window10\"]).count().describe()\n",
    "# We are sampling with replacement, which should be okay since it is within a second\n",
    "acc_data_pid_20s = acc_data_pid.groupby([ \"pid\", \"time\"]).sample(n = 20, replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Make this as an assert statement in the begininng for both tac and accelerometer data\n",
    "acc_data_pid_20s[\"time\"].is_monotonic_increasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SLIDING WINDOW\n",
    "acc_data_sliding = acc_data_pid_20s.copy()\n",
    "# cols = [\"x\", \"y\", \"z\"]\n",
    "# window_size = 10 # including current\n",
    "\n",
    "# for col in cols:\n",
    "#     cols_to_append = []\n",
    "#     for i in range(0, window_size):\n",
    "#         shifted_col_name = str(col) + \"_\" + str(i)\n",
    "#         acc_data_sliding[shifted_col_name] = acc_data_sliding[col].shift(i, fill_value = 0)\n",
    "#         cols_to_append.append(shifted_col_name)\n",
    "    \n",
    "#     # we have (windpw_size ) columsn for col\n",
    "#     # Uncomment to keep original columns\n",
    "#     acc_data_sliding = acc_data_sliding.drop(columns=[col])    \n",
    "#     acc_data_sliding[str(col)] = acc_data_sliding[cols_to_append].values.tolist()\n",
    "    \n",
    "#     # acc_data_sliding = acc_data_sliding.drop(columns=cols_to_append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # x_sliding_window.shape\n",
    "pids = [\"BK7610\"]\n",
    "final = []\n",
    "labels = []\n",
    "for pid in pids:\n",
    "  # temptemp = acc_data_pid_20s[acc_data_pid_20s['pid'] == pid]\n",
    "  temptemp = acc_data_sliding[acc_data_sliding['pid'] == pid]\n",
    "  times = temptemp.time.unique()\n",
    "  final_temp =[]\n",
    "  labels_temp = []\n",
    "  for i in range(len(times)):\n",
    "    # x = np.lib.stride_tricks.sliding_window_view(frame_temp2[frame_temp2.pid == pid and frame_temp2.window10 == time], window_shape = 10)\n",
    "    # temptemptemp = temptemp[temptemp['time'] == time]\n",
    "    time_to_filter = [times[j] if j >= 0 else -1 for j in range(i-9, i+1)]\n",
    "    # print(time_to_filter)\n",
    "    # if i == 10:\n",
    "    #   break\n",
    "    temptemptemp = temptemp[temptemp['time'].isin(time_to_filter)]\n",
    "    # TODO: Create x y z sliding windows\n",
    "    # x = np.lib.stride_tricks.sliding_window_view(temptemptemp[\"x\"], window_shape = 10)\n",
    "    # y = np.lib.stride_tricks.sliding_window_view(temptemptemp[\"y\"], window_shape = 10)\n",
    "    # z = np.lib.stride_tricks.sliding_window_view(temptemptemp[\"z\"], window_shape = 10)\n",
    "\n",
    "    # x_dash = np.array(temptemptemp[[\"x_\"+str(i) for i in range(window_size)]]).flatten()\n",
    "    # y_dash = np.array(temptemptemp[[\"y_\"+str(i) for i in range(window_size)]]).flatten()\n",
    "    # z_dash = np.array(temptemptemp[[\"z_\"+str(i) for i in range(window_size)]]).flatten()\n",
    "\n",
    "    x_dash = np.array(temptemptemp[\"x\"])\n",
    "    y_dash = np.array(temptemptemp[\"y\"])\n",
    "    z_dash = np.array(temptemptemp[\"z\"])\n",
    "\n",
    "    x_dash = np.pad(x_dash, (200 - len(x_dash), 0), \"constant\")\n",
    "    y_dash = np.pad(y_dash, (200 - len(y_dash), 0), \"constant\")\n",
    "    z_dash = np.pad(z_dash, (200 - len(z_dash), 0), \"constant\")\n",
    "\n",
    "    # a = np.vstack((temptemptemp[\"x\"].apply(lambda x: np.array(x, dtype=\"float32\")), temptemptemp[\"y\"].apply(lambda x: np.array(x, dtype=\"float32\")), temptemptemp[\"z\"].apply(lambda x: np.array(x, dtype=\"float32\"))))\n",
    "    a = np.transpose(np.vstack((x_dash, y_dash, z_dash)))\n",
    "    final_temp.append(a)\n",
    "    labels_temp.append(temptemptemp.head(1)[\"tac\"])\n",
    "  final.append(np.array(final_temp))\n",
    "  labels.append(np.array(labels_temp))\n",
    "  # print(final)\n",
    "  \n",
    "  # break\n",
    "# print(np.array(final,dtype=object).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>time</th>\n",
       "      <th>tac</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47140</th>\n",
       "      <td>BK7610</td>\n",
       "      <td>0.1443</td>\n",
       "      <td>-0.0474</td>\n",
       "      <td>-0.0447</td>\n",
       "      <td>1493735870</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47152</th>\n",
       "      <td>BK7610</td>\n",
       "      <td>0.1133</td>\n",
       "      <td>0.0080</td>\n",
       "      <td>-0.0172</td>\n",
       "      <td>1493735870</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47140</th>\n",
       "      <td>BK7610</td>\n",
       "      <td>0.1443</td>\n",
       "      <td>-0.0474</td>\n",
       "      <td>-0.0447</td>\n",
       "      <td>1493735870</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47146</th>\n",
       "      <td>BK7610</td>\n",
       "      <td>0.1155</td>\n",
       "      <td>-0.0284</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1493735870</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47138</th>\n",
       "      <td>BK7610</td>\n",
       "      <td>0.1336</td>\n",
       "      <td>-0.0697</td>\n",
       "      <td>-0.0446</td>\n",
       "      <td>1493735870</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47856</th>\n",
       "      <td>BK7610</td>\n",
       "      <td>-0.0535</td>\n",
       "      <td>-0.0496</td>\n",
       "      <td>-0.0077</td>\n",
       "      <td>1493735879</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47804</th>\n",
       "      <td>BK7610</td>\n",
       "      <td>-0.0108</td>\n",
       "      <td>-0.0559</td>\n",
       "      <td>0.0268</td>\n",
       "      <td>1493735879</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47852</th>\n",
       "      <td>BK7610</td>\n",
       "      <td>-0.0672</td>\n",
       "      <td>-0.0969</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>1493735879</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47854</th>\n",
       "      <td>BK7610</td>\n",
       "      <td>-0.0636</td>\n",
       "      <td>-0.0894</td>\n",
       "      <td>0.0330</td>\n",
       "      <td>1493735879</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47810</th>\n",
       "      <td>BK7610</td>\n",
       "      <td>0.0384</td>\n",
       "      <td>0.0236</td>\n",
       "      <td>-0.1074</td>\n",
       "      <td>1493735879</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          pid       x       y       z        time  tac\n",
       "47140  BK7610  0.1443 -0.0474 -0.0447  1493735870    0\n",
       "47152  BK7610  0.1133  0.0080 -0.0172  1493735870    0\n",
       "47140  BK7610  0.1443 -0.0474 -0.0447  1493735870    0\n",
       "47146  BK7610  0.1155 -0.0284  0.0000  1493735870    0\n",
       "47138  BK7610  0.1336 -0.0697 -0.0446  1493735870    0\n",
       "...       ...     ...     ...     ...         ...  ...\n",
       "47856  BK7610 -0.0535 -0.0496 -0.0077  1493735879    0\n",
       "47804  BK7610 -0.0108 -0.0559  0.0268  1493735879    0\n",
       "47852  BK7610 -0.0672 -0.0969  0.0055  1493735879    0\n",
       "47854  BK7610 -0.0636 -0.0894  0.0330  1493735879    0\n",
       "47810  BK7610  0.0384  0.0236 -0.1074  1493735879    0\n",
       "\n",
       "[200 rows x 6 columns]"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# temptemptemp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 3)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [-0.1331,  0.026 ,  0.1165, -0.0039, -0.1326, -0.1095,  0.1165,\n",
    "        #  0.1467, -0.1494, -0.0233]\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 30735, 200, 3)\n",
      "(1, 30735, 1)\n"
     ]
    }
   ],
   "source": [
    "final_arr = np.asarray(final).astype('float32')\n",
    "\n",
    "# final_arr = np.reshape(final_arr, (final_arr.shape[0], final_arr.shape[1], final_arr.shape[3], final_arr.shape[2]))\n",
    "\n",
    "labels_arr = np.asarray(labels).astype('float32')\n",
    "print(final_arr.shape)\n",
    "print(labels_arr.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final_arr[0][9][0:19] == final_arr[0][0][180:199]\n",
    "# np.unique(labels_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30735, 200, 3), (30735, 1))"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_arr_reshape = np.reshape(final_arr, (30735, 200, 3)) \n",
    "labels_arr_reshape = np.reshape(labels_arr, (30735,1))\n",
    "final_arr_reshape.shape, labels_arr_reshape.shape \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = range(len(final_arr_reshape))\n",
    "indices = tf.random.shuffle(indices)\n",
    "\n",
    "final_arr_reshape = tf.gather(final_arr_reshape, indices)\n",
    "labels_arr_reshape = tf.gather(labels_arr_reshape, indices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from TF_Model import Drunk\n",
    "# drunk = Drunk()\n",
    "\n",
    "# drunk.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), \\\n",
    "#                    optimizer=tf.keras.optimizers.Adam(0.03), \\\n",
    "#                    metrics=[tf.keras.metrics.Accuracy()]) \n",
    "\n",
    "# drunk.build((30735, 600))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model \n",
    "#### 3 hidden layers (32 nodes, RELU, 32 nodes, RELU, 16 nodes, RELU)  \n",
    "#### output layer uses sigmoid activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_77\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_75 (Flatten)        (30735, 600)              0         \n",
      "                                                                 \n",
      " dense_242 (Dense)           (30735, 1024)             615424    \n",
      "                                                                 \n",
      " dense_243 (Dense)           (30735, 768)              787200    \n",
      "                                                                 \n",
      " dense_244 (Dense)           (30735, 512)              393728    \n",
      "                                                                 \n",
      " dense_245 (Dense)           (30735, 256)              131328    \n",
      "                                                                 \n",
      " dense_246 (Dense)           (30735, 128)              32896     \n",
      "                                                                 \n",
      " dense_247 (Dense)           (30735, 64)               8256      \n",
      "                                                                 \n",
      " dense_248 (Dense)           (30735, 32)               2080      \n",
      "                                                                 \n",
      " dense_249 (Dense)           (30735, 1)                33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,970,945\n",
      "Trainable params: 1,970,945\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model_data = np.reshape(final_arr, (30735, 200, 3)) \n",
    "base_model_labels = np.reshape(labels_arr, (30735,1))\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "flatten = tf.keras.layers.Flatten()\n",
    "fc_layer1 = tf.keras.layers.Dense(units=1024, activation = 'relu')\n",
    "fc_layer2 = tf.keras.layers.Dense(units=768, activation = 'relu')\n",
    "fc_layer3 = tf.keras.layers.Dense(units=512, activation = 'relu')\n",
    "fc_layer4 = tf.keras.layers.Dense(units=256, activation = 'relu')\n",
    "fc_layer5 = tf.keras.layers.Dense(units=128, activation = 'relu')\n",
    "fc_layer6 = tf.keras.layers.Dense(units=64, activation = 'relu')\n",
    "fc_layer7 = tf.keras.layers.Dense(units=32, activation = 'relu')\n",
    "fc_layer8 = tf.keras.layers.Dense(units=1, activation = 'sigmoid')\n",
    "# Cant use softmax at the end since it will normalize and give 1 \n",
    "base_model = tf.keras.Sequential([\n",
    "    flatten,\n",
    " fc_layer1, fc_layer2, \n",
    "fc_layer3, \n",
    "fc_layer4,\n",
    "fc_layer5,\n",
    "fc_layer6,\n",
    "fc_layer7,\n",
    "fc_layer8\n",
    "])\n",
    "\n",
    "base_model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits = False), \\\n",
    "                   optimizer=tf.keras.optimizers.Adam(0.001, beta_1=0.9, beta_2= 0.999), \\\n",
    "                   metrics=[tf.keras.metrics.Accuracy()]) \n",
    "\n",
    "base_model.build((30735, 200, 3))\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.6301 - accuracy: 0.0000e+00\n",
      "Epoch 2/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.5771 - accuracy: 0.0000e+00\n",
      "Epoch 3/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.5198 - accuracy: 0.0000e+00\n",
      "Epoch 4/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.4204 - accuracy: 0.0025\n",
      "Epoch 5/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.3092 - accuracy: 0.0143\n",
      "Epoch 6/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.2389 - accuracy: 0.0266\n",
      "Epoch 7/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.1961 - accuracy: 0.0657\n",
      "Epoch 8/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.1744 - accuracy: 0.0741\n",
      "Epoch 9/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.1788 - accuracy: 0.0883\n",
      "Epoch 10/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.1524 - accuracy: 0.1073\n",
      "Epoch 11/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.1360 - accuracy: 0.1187\n",
      "Epoch 12/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.1350 - accuracy: 0.1259\n",
      "Epoch 13/1000\n",
      "121/121 [==============================] - 2s 16ms/step - loss: 0.1299 - accuracy: 0.1124\n",
      "Epoch 14/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.1116 - accuracy: 0.1218\n",
      "Epoch 15/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.1104 - accuracy: 0.1330\n",
      "Epoch 16/1000\n",
      "121/121 [==============================] - 2s 16ms/step - loss: 0.1063 - accuracy: 0.1630\n",
      "Epoch 17/1000\n",
      "121/121 [==============================] - 2s 16ms/step - loss: 0.1145 - accuracy: 0.1441\n",
      "Epoch 18/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0990 - accuracy: 0.1827\n",
      "Epoch 19/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.1086 - accuracy: 0.1712\n",
      "Epoch 20/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0996 - accuracy: 0.1777\n",
      "Epoch 21/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0940 - accuracy: 0.1995\n",
      "Epoch 22/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0889 - accuracy: 0.1929\n",
      "Epoch 23/1000\n",
      "121/121 [==============================] - 2s 16ms/step - loss: 0.0867 - accuracy: 0.2061\n",
      "Epoch 24/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.1061 - accuracy: 0.1368\n",
      "Epoch 25/1000\n",
      "121/121 [==============================] - 2s 16ms/step - loss: 0.0896 - accuracy: 0.1612\n",
      "Epoch 26/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0793 - accuracy: 0.2508\n",
      "Epoch 27/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0780 - accuracy: 0.2432\n",
      "Epoch 28/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0744 - accuracy: 0.2199\n",
      "Epoch 29/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0695 - accuracy: 0.2445\n",
      "Epoch 30/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0757 - accuracy: 0.2284\n",
      "Epoch 31/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0698 - accuracy: 0.2260\n",
      "Epoch 32/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0723 - accuracy: 0.2909\n",
      "Epoch 33/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0710 - accuracy: 0.2261\n",
      "Epoch 34/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0670 - accuracy: 0.2663\n",
      "Epoch 35/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0790 - accuracy: 0.2753\n",
      "Epoch 36/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0713 - accuracy: 0.2616\n",
      "Epoch 37/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0717 - accuracy: 0.2351\n",
      "Epoch 38/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0639 - accuracy: 0.2246\n",
      "Epoch 39/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0574 - accuracy: 0.2589\n",
      "Epoch 40/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0635 - accuracy: 0.2442\n",
      "Epoch 41/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0677 - accuracy: 0.2525\n",
      "Epoch 42/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0775 - accuracy: 0.2905\n",
      "Epoch 43/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0743 - accuracy: 0.2629\n",
      "Epoch 44/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0642 - accuracy: 0.2634\n",
      "Epoch 45/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0543 - accuracy: 0.3193\n",
      "Epoch 46/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0702 - accuracy: 0.2269\n",
      "Epoch 47/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0572 - accuracy: 0.2652\n",
      "Epoch 48/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0519 - accuracy: 0.3623\n",
      "Epoch 49/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0553 - accuracy: 0.3769\n",
      "Epoch 50/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0549 - accuracy: 0.3482\n",
      "Epoch 51/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0500 - accuracy: 0.3490\n",
      "Epoch 52/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0494 - accuracy: 0.3479\n",
      "Epoch 53/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0476 - accuracy: 0.3776\n",
      "Epoch 54/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0466 - accuracy: 0.3349\n",
      "Epoch 55/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0521 - accuracy: 0.3631\n",
      "Epoch 56/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0669 - accuracy: 0.2198\n",
      "Epoch 57/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0511 - accuracy: 0.2791\n",
      "Epoch 58/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0487 - accuracy: 0.2823\n",
      "Epoch 59/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0554 - accuracy: 0.2697\n",
      "Epoch 60/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0474 - accuracy: 0.3099\n",
      "Epoch 61/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0445 - accuracy: 0.3959\n",
      "Epoch 62/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0454 - accuracy: 0.3615\n",
      "Epoch 63/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0416 - accuracy: 0.3397\n",
      "Epoch 64/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0467 - accuracy: 0.3810\n",
      "Epoch 65/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0408 - accuracy: 0.4304\n",
      "Epoch 66/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0595 - accuracy: 0.2242\n",
      "Epoch 67/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0461 - accuracy: 0.2349\n",
      "Epoch 68/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0482 - accuracy: 0.2915\n",
      "Epoch 69/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0421 - accuracy: 0.3488\n",
      "Epoch 70/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0547 - accuracy: 0.3143\n",
      "Epoch 71/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0456 - accuracy: 0.3097\n",
      "Epoch 72/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0456 - accuracy: 0.3723\n",
      "Epoch 73/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0477 - accuracy: 0.3245\n",
      "Epoch 74/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0416 - accuracy: 0.2849\n",
      "Epoch 75/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0523 - accuracy: 0.3484\n",
      "Epoch 76/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0440 - accuracy: 0.3697\n",
      "Epoch 77/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0448 - accuracy: 0.3198\n",
      "Epoch 78/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0488 - accuracy: 0.3177\n",
      "Epoch 79/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0380 - accuracy: 0.4163\n",
      "Epoch 80/1000\n",
      "121/121 [==============================] - 2s 17ms/step - loss: 0.0390 - accuracy: 0.4141\n",
      "Epoch 81/1000\n",
      "121/121 [==============================] - 2s 16ms/step - loss: 0.0434 - accuracy: 0.3059\n",
      "Epoch 82/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0411 - accuracy: 0.3189\n",
      "Epoch 83/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0359 - accuracy: 0.3635\n",
      "Epoch 84/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0410 - accuracy: 0.3869\n",
      "Epoch 85/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0346 - accuracy: 0.3810\n",
      "Epoch 86/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0340 - accuracy: 0.3928\n",
      "Epoch 87/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0403 - accuracy: 0.3999\n",
      "Epoch 88/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0373 - accuracy: 0.3322\n",
      "Epoch 89/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0344 - accuracy: 0.4453\n",
      "Epoch 90/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0461 - accuracy: 0.4090\n",
      "Epoch 91/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0377 - accuracy: 0.3468\n",
      "Epoch 92/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0359 - accuracy: 0.4211\n",
      "Epoch 93/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0358 - accuracy: 0.3969\n",
      "Epoch 94/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0450 - accuracy: 0.3357\n",
      "Epoch 95/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0376 - accuracy: 0.4349\n",
      "Epoch 96/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0309 - accuracy: 0.4657\n",
      "Epoch 97/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0373 - accuracy: 0.3713\n",
      "Epoch 98/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0270 - accuracy: 0.4336\n",
      "Epoch 99/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0426 - accuracy: 0.3796\n",
      "Epoch 100/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0329 - accuracy: 0.3401\n",
      "Epoch 101/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0333 - accuracy: 0.3709\n",
      "Epoch 102/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0298 - accuracy: 0.4407\n",
      "Epoch 103/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0280 - accuracy: 0.4652\n",
      "Epoch 104/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0314 - accuracy: 0.4509\n",
      "Epoch 105/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0306 - accuracy: 0.4534\n",
      "Epoch 106/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0270 - accuracy: 0.4585\n",
      "Epoch 107/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0280 - accuracy: 0.4810\n",
      "Epoch 108/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0256 - accuracy: 0.4908\n",
      "Epoch 109/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0309 - accuracy: 0.4784\n",
      "Epoch 110/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0250 - accuracy: 0.4788\n",
      "Epoch 111/1000\n",
      "121/121 [==============================] - 2s 16ms/step - loss: 0.0287 - accuracy: 0.4897\n",
      "Epoch 112/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0301 - accuracy: 0.4827\n",
      "Epoch 113/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0264 - accuracy: 0.4985\n",
      "Epoch 114/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0268 - accuracy: 0.4950\n",
      "Epoch 115/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0205 - accuracy: 0.5085\n",
      "Epoch 116/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0214 - accuracy: 0.5198\n",
      "Epoch 117/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0296 - accuracy: 0.5068\n",
      "Epoch 118/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0352 - accuracy: 0.4867\n",
      "Epoch 119/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0226 - accuracy: 0.5333\n",
      "Epoch 120/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0242 - accuracy: 0.5422\n",
      "Epoch 121/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0201 - accuracy: 0.5436\n",
      "Epoch 122/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0378 - accuracy: 0.4954\n",
      "Epoch 123/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0618 - accuracy: 0.2454\n",
      "Epoch 124/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0406 - accuracy: 0.3074\n",
      "Epoch 125/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0290 - accuracy: 0.3426\n",
      "Epoch 126/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0314 - accuracy: 0.3271\n",
      "Epoch 127/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0218 - accuracy: 0.4270\n",
      "Epoch 128/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0254 - accuracy: 0.4634\n",
      "Epoch 129/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0217 - accuracy: 0.4863\n",
      "Epoch 130/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0223 - accuracy: 0.4854\n",
      "Epoch 131/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0214 - accuracy: 0.4942\n",
      "Epoch 132/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0450 - accuracy: 0.4508\n",
      "Epoch 133/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0229 - accuracy: 0.5029\n",
      "Epoch 134/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0215 - accuracy: 0.5130\n",
      "Epoch 135/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0207 - accuracy: 0.5233\n",
      "Epoch 136/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0358 - accuracy: 0.5050\n",
      "Epoch 137/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0211 - accuracy: 0.5312\n",
      "Epoch 138/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0187 - accuracy: 0.5354\n",
      "Epoch 139/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0200 - accuracy: 0.5358\n",
      "Epoch 140/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0165 - accuracy: 0.5433\n",
      "Epoch 141/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0217 - accuracy: 0.5403\n",
      "Epoch 142/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0180 - accuracy: 0.5487\n",
      "Epoch 143/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0165 - accuracy: 0.5491\n",
      "Epoch 144/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0215 - accuracy: 0.5084\n",
      "Epoch 145/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0442 - accuracy: 0.3303\n",
      "Epoch 146/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0393 - accuracy: 0.2728\n",
      "Epoch 147/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0196 - accuracy: 0.4087\n",
      "Epoch 148/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0217 - accuracy: 0.4453\n",
      "Epoch 149/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0224 - accuracy: 0.4302\n",
      "Epoch 150/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0190 - accuracy: 0.4282\n",
      "Epoch 151/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0178 - accuracy: 0.4493\n",
      "Epoch 152/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0189 - accuracy: 0.4610\n",
      "Epoch 153/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0153 - accuracy: 0.4544\n",
      "Epoch 154/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0166 - accuracy: 0.4836\n",
      "Epoch 155/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0178 - accuracy: 0.4346\n",
      "Epoch 156/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0168 - accuracy: 0.4228\n",
      "Epoch 157/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0138 - accuracy: 0.4548\n",
      "Epoch 158/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0135 - accuracy: 0.5025\n",
      "Epoch 159/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0166 - accuracy: 0.4790\n",
      "Epoch 160/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0193 - accuracy: 0.4727\n",
      "Epoch 161/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0256 - accuracy: 0.4292\n",
      "Epoch 162/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0210 - accuracy: 0.3738\n",
      "Epoch 163/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0186 - accuracy: 0.3849\n",
      "Epoch 164/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0357 - accuracy: 0.4335\n",
      "Epoch 165/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0277 - accuracy: 0.4178\n",
      "Epoch 166/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0156 - accuracy: 0.4448\n",
      "Epoch 167/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0143 - accuracy: 0.4568\n",
      "Epoch 168/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0211 - accuracy: 0.4961\n",
      "Epoch 169/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0103 - accuracy: 0.5204\n",
      "Epoch 170/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0166 - accuracy: 0.5151\n",
      "Epoch 171/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0145 - accuracy: 0.5347\n",
      "Epoch 172/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0137 - accuracy: 0.5404\n",
      "Epoch 173/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0095 - accuracy: 0.5436\n",
      "Epoch 174/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0105 - accuracy: 0.5359\n",
      "Epoch 175/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0169 - accuracy: 0.5368\n",
      "Epoch 176/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0115 - accuracy: 0.5343\n",
      "Epoch 177/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0126 - accuracy: 0.5460\n",
      "Epoch 178/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0133 - accuracy: 0.5436\n",
      "Epoch 179/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0126 - accuracy: 0.5617\n",
      "Epoch 180/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0143 - accuracy: 0.5471\n",
      "Epoch 181/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0088 - accuracy: 0.5677\n",
      "Epoch 182/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0119 - accuracy: 0.5523\n",
      "Epoch 183/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0179 - accuracy: 0.5512\n",
      "Epoch 184/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0147 - accuracy: 0.5451\n",
      "Epoch 185/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0153 - accuracy: 0.4433\n",
      "Epoch 186/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0246 - accuracy: 0.4044\n",
      "Epoch 187/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0224 - accuracy: 0.3763\n",
      "Epoch 188/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0142 - accuracy: 0.4699\n",
      "Epoch 189/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0119 - accuracy: 0.4931\n",
      "Epoch 190/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0236 - accuracy: 0.4910\n",
      "Epoch 191/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0112 - accuracy: 0.4727\n",
      "Epoch 192/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0060 - accuracy: 0.4983\n",
      "Epoch 193/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0243 - accuracy: 0.5055\n",
      "Epoch 194/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0172 - accuracy: 0.4470\n",
      "Epoch 195/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0191 - accuracy: 0.4099\n",
      "Epoch 196/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0194 - accuracy: 0.3822\n",
      "Epoch 197/1000\n",
      "121/121 [==============================] - 2s 16ms/step - loss: 0.0175 - accuracy: 0.4213\n",
      "Epoch 198/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0093 - accuracy: 0.4837\n",
      "Epoch 199/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0115 - accuracy: 0.5317\n",
      "Epoch 200/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0081 - accuracy: 0.5292\n",
      "Epoch 201/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0279 - accuracy: 0.5086\n",
      "Epoch 202/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0105 - accuracy: 0.5625\n",
      "Epoch 203/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0135 - accuracy: 0.5580\n",
      "Epoch 204/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0165 - accuracy: 0.5375\n",
      "Epoch 205/1000\n",
      "121/121 [==============================] - 2s 16ms/step - loss: 0.0094 - accuracy: 0.5692\n",
      "Epoch 206/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0082 - accuracy: 0.5881\n",
      "Epoch 207/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0103 - accuracy: 0.5904\n",
      "Epoch 208/1000\n",
      "121/121 [==============================] - 2s 16ms/step - loss: 0.0052 - accuracy: 0.6026\n",
      "Epoch 209/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0102 - accuracy: 0.6066\n",
      "Epoch 210/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0200 - accuracy: 0.4364\n",
      "Epoch 211/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0194 - accuracy: 0.3697\n",
      "Epoch 212/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0165 - accuracy: 0.4661\n",
      "Epoch 213/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0142 - accuracy: 0.4940\n",
      "Epoch 214/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0089 - accuracy: 0.5347\n",
      "Epoch 215/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0110 - accuracy: 0.4668\n",
      "Epoch 216/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0187 - accuracy: 0.3788\n",
      "Epoch 217/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0133 - accuracy: 0.4473\n",
      "Epoch 218/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0116 - accuracy: 0.4818\n",
      "Epoch 219/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0077 - accuracy: 0.5104\n",
      "Epoch 220/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0097 - accuracy: 0.5064\n",
      "Epoch 221/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0102 - accuracy: 0.5376\n",
      "Epoch 222/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0060 - accuracy: 0.5846\n",
      "Epoch 223/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0111 - accuracy: 0.5738\n",
      "Epoch 224/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0087 - accuracy: 0.6020\n",
      "Epoch 225/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0056 - accuracy: 0.6048\n",
      "Epoch 226/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0047 - accuracy: 0.5816\n",
      "Epoch 227/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0137 - accuracy: 0.4537\n",
      "Epoch 228/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0380 - accuracy: 0.4563\n",
      "Epoch 229/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0149 - accuracy: 0.4144\n",
      "Epoch 230/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0149 - accuracy: 0.4824\n",
      "Epoch 231/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0067 - accuracy: 0.5209\n",
      "Epoch 232/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0216 - accuracy: 0.4026\n",
      "Epoch 233/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0093 - accuracy: 0.4603\n",
      "Epoch 234/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0074 - accuracy: 0.5035\n",
      "Epoch 235/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0207 - accuracy: 0.5001\n",
      "Epoch 236/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0132 - accuracy: 0.5239\n",
      "Epoch 237/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0070 - accuracy: 0.5541\n",
      "Epoch 238/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0120 - accuracy: 0.5617\n",
      "Epoch 239/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0157 - accuracy: 0.5566\n",
      "Epoch 240/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0105 - accuracy: 0.5665\n",
      "Epoch 241/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0091 - accuracy: 0.5862\n",
      "Epoch 242/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0088 - accuracy: 0.5808\n",
      "Epoch 243/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0098 - accuracy: 0.5807\n",
      "Epoch 244/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0115 - accuracy: 0.5946\n",
      "Epoch 245/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0057 - accuracy: 0.6018\n",
      "Epoch 246/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0079 - accuracy: 0.6180\n",
      "Epoch 247/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0126 - accuracy: 0.5712\n",
      "Epoch 248/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0096 - accuracy: 0.5697\n",
      "Epoch 249/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0077 - accuracy: 0.4965\n",
      "Epoch 250/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0162 - accuracy: 0.4392\n",
      "Epoch 251/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0062 - accuracy: 0.4970\n",
      "Epoch 252/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0054 - accuracy: 0.4863\n",
      "Epoch 253/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0098 - accuracy: 0.5084\n",
      "Epoch 254/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0398 - accuracy: 0.4467\n",
      "Epoch 255/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0159 - accuracy: 0.4039\n",
      "Epoch 256/1000\n",
      "121/121 [==============================] - 3s 25ms/step - loss: 0.0095 - accuracy: 0.4826\n",
      "Epoch 257/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0158 - accuracy: 0.4223\n",
      "Epoch 258/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0161 - accuracy: 0.4939\n",
      "Epoch 259/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0104 - accuracy: 0.5096\n",
      "Epoch 260/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0204 - accuracy: 0.4971\n",
      "Epoch 261/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0086 - accuracy: 0.5332\n",
      "Epoch 262/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0172 - accuracy: 0.5067\n",
      "Epoch 263/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0099 - accuracy: 0.4984\n",
      "Epoch 264/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0108 - accuracy: 0.4939\n",
      "Epoch 265/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0054 - accuracy: 0.5367\n",
      "Epoch 266/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0073 - accuracy: 0.5501\n",
      "Epoch 267/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0057 - accuracy: 0.5484\n",
      "Epoch 268/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0093 - accuracy: 0.5477\n",
      "Epoch 269/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0052 - accuracy: 0.5603\n",
      "Epoch 270/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0121 - accuracy: 0.5360\n",
      "Epoch 271/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0088 - accuracy: 0.5484\n",
      "Epoch 272/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0047 - accuracy: 0.5550\n",
      "Epoch 273/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0029 - accuracy: 0.5911\n",
      "Epoch 274/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0122 - accuracy: 0.5691\n",
      "Epoch 275/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0097 - accuracy: 0.5601\n",
      "Epoch 276/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0117 - accuracy: 0.5190\n",
      "Epoch 277/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0042 - accuracy: 0.5531\n",
      "Epoch 278/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0092 - accuracy: 0.5508\n",
      "Epoch 279/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0030 - accuracy: 0.5803\n",
      "Epoch 280/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0116 - accuracy: 0.5601\n",
      "Epoch 281/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0028 - accuracy: 0.5911\n",
      "Epoch 282/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0076 - accuracy: 0.5900\n",
      "Epoch 283/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0033 - accuracy: 0.5840\n",
      "Epoch 284/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0143 - accuracy: 0.5734\n",
      "Epoch 285/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0189 - accuracy: 0.4502\n",
      "Epoch 286/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0262 - accuracy: 0.3656\n",
      "Epoch 287/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0193 - accuracy: 0.4430\n",
      "Epoch 288/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0295 - accuracy: 0.4588\n",
      "Epoch 289/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0152 - accuracy: 0.3275\n",
      "Epoch 290/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0086 - accuracy: 0.4019\n",
      "Epoch 291/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0038 - accuracy: 0.4421\n",
      "Epoch 292/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0184 - accuracy: 0.4577\n",
      "Epoch 293/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0035 - accuracy: 0.5389\n",
      "Epoch 294/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0082 - accuracy: 0.5427\n",
      "Epoch 295/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0015 - accuracy: 0.5608\n",
      "Epoch 296/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0010 - accuracy: 0.5861\n",
      "Epoch 297/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0070 - accuracy: 0.5748\n",
      "Epoch 298/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0021 - accuracy: 0.5800\n",
      "Epoch 299/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0177 - accuracy: 0.5789\n",
      "Epoch 300/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0072 - accuracy: 0.5720\n",
      "Epoch 301/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0013 - accuracy: 0.6314\n",
      "Epoch 302/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0094 - accuracy: 0.6182\n",
      "Epoch 303/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0059 - accuracy: 0.6053\n",
      "Epoch 304/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0298 - accuracy: 0.6033\n",
      "Epoch 305/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0096 - accuracy: 0.6373\n",
      "Epoch 306/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0070 - accuracy: 0.3828\n",
      "Epoch 307/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0045 - accuracy: 0.4552\n",
      "Epoch 308/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0036 - accuracy: 0.5369\n",
      "Epoch 309/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0088 - accuracy: 0.5526\n",
      "Epoch 310/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0100 - accuracy: 0.5418\n",
      "Epoch 311/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0074 - accuracy: 0.5427\n",
      "Epoch 312/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0135 - accuracy: 0.5320\n",
      "Epoch 313/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0051 - accuracy: 0.5489\n",
      "Epoch 314/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0013 - accuracy: 0.5573\n",
      "Epoch 315/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0031 - accuracy: 0.5731\n",
      "Epoch 316/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0033 - accuracy: 0.5723\n",
      "Epoch 317/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0080 - accuracy: 0.5742\n",
      "Epoch 318/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0030 - accuracy: 0.5607\n",
      "Epoch 319/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 5.8311e-04 - accuracy: 0.5826\n",
      "Epoch 320/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0072 - accuracy: 0.5922\n",
      "Epoch 321/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0083 - accuracy: 0.5501\n",
      "Epoch 322/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0055 - accuracy: 0.5630\n",
      "Epoch 323/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0022 - accuracy: 0.5769\n",
      "Epoch 324/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 5.5869e-04 - accuracy: 0.5956\n",
      "Epoch 325/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 3.8983e-04 - accuracy: 0.6062\n",
      "Epoch 326/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 4.6958e-04 - accuracy: 0.6239\n",
      "Epoch 327/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0157 - accuracy: 0.5514\n",
      "Epoch 328/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0091 - accuracy: 0.5498\n",
      "Epoch 329/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0172 - accuracy: 0.4950\n",
      "Epoch 330/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0159 - accuracy: 0.4213\n",
      "Epoch 331/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0191 - accuracy: 0.4519\n",
      "Epoch 332/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0049 - accuracy: 0.4944\n",
      "Epoch 333/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0116 - accuracy: 0.5110\n",
      "Epoch 334/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0137 - accuracy: 0.4866\n",
      "Epoch 335/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0029 - accuracy: 0.5413\n",
      "Epoch 336/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0017 - accuracy: 0.4959\n",
      "Epoch 337/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 3.0576e-04 - accuracy: 0.5212\n",
      "Epoch 338/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0041 - accuracy: 0.5428\n",
      "Epoch 339/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0498 - accuracy: 0.4599\n",
      "Epoch 340/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0203 - accuracy: 0.5216\n",
      "Epoch 341/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0161 - accuracy: 0.5550\n",
      "Epoch 342/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0141 - accuracy: 0.5529\n",
      "Epoch 343/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0126 - accuracy: 0.5743\n",
      "Epoch 344/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0124 - accuracy: 0.5959\n",
      "Epoch 345/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0076 - accuracy: 0.5953\n",
      "Epoch 346/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0111 - accuracy: 0.6294\n",
      "Epoch 347/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0068 - accuracy: 0.5950\n",
      "Epoch 348/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0103 - accuracy: 0.6259\n",
      "Epoch 349/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0049 - accuracy: 0.6231\n",
      "Epoch 350/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0036 - accuracy: 0.6741\n",
      "Epoch 351/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0556 - accuracy: 0.5290\n",
      "Epoch 352/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0332 - accuracy: 0.4730\n",
      "Epoch 353/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0282 - accuracy: 0.4102\n",
      "Epoch 354/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0212 - accuracy: 0.4659\n",
      "Epoch 355/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0386 - accuracy: 0.4525\n",
      "Epoch 356/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0200 - accuracy: 0.4440\n",
      "Epoch 357/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0146 - accuracy: 0.4643\n",
      "Epoch 358/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0134 - accuracy: 0.5209\n",
      "Epoch 359/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0121 - accuracy: 0.5267\n",
      "Epoch 360/1000\n",
      "121/121 [==============================] - 2s 16ms/step - loss: 0.0092 - accuracy: 0.5516\n",
      "Epoch 361/1000\n",
      "121/121 [==============================] - 3s 22ms/step - loss: 0.0160 - accuracy: 0.5475\n",
      "Epoch 362/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0165 - accuracy: 0.5528\n",
      "Epoch 363/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0111 - accuracy: 0.5428\n",
      "Epoch 364/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0087 - accuracy: 0.5566\n",
      "Epoch 365/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0126 - accuracy: 0.5539\n",
      "Epoch 366/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0067 - accuracy: 0.5585\n",
      "Epoch 367/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0059 - accuracy: 0.5654\n",
      "Epoch 368/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0084 - accuracy: 0.5704\n",
      "Epoch 369/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0060 - accuracy: 0.5776\n",
      "Epoch 370/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0098 - accuracy: 0.5589\n",
      "Epoch 371/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0096 - accuracy: 0.5702\n",
      "Epoch 372/1000\n",
      "121/121 [==============================] - 2s 15ms/step - loss: 0.0096 - accuracy: 0.5342\n",
      "Epoch 373/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0150 - accuracy: 0.4680\n",
      "Epoch 374/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0135 - accuracy: 0.4511\n",
      "Epoch 375/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0091 - accuracy: 0.5028\n",
      "Epoch 376/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0112 - accuracy: 0.5044\n",
      "Epoch 377/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0049 - accuracy: 0.5342\n",
      "Epoch 378/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0124 - accuracy: 0.5332\n",
      "Epoch 379/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0038 - accuracy: 0.5537\n",
      "Epoch 380/1000\n",
      "121/121 [==============================] - 2s 13ms/step - loss: 0.0086 - accuracy: 0.5663\n",
      "Epoch 381/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0088 - accuracy: 0.5370\n",
      "Epoch 382/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0059 - accuracy: 0.5478\n",
      "Epoch 383/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0033 - accuracy: 0.5766\n",
      "Epoch 384/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0137 - accuracy: 0.5441\n",
      "Epoch 385/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0043 - accuracy: 0.5640\n",
      "Epoch 386/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0052 - accuracy: 0.5660\n",
      "Epoch 387/1000\n",
      "121/121 [==============================] - 2s 14ms/step - loss: 0.0065 - accuracy: 0.5796\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x367281bd0>"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=100)\n",
    "base_model.fit(base_model_data, base_model_labels, epochs = 1000,\n",
    "               batch_size = 256, \n",
    "               verbose=1, \n",
    "               callbacks=[callback]) \n",
    "\n",
    "# Paper baseline model as-it-is\n",
    "#                Epoch 500/500\n",
    "# 961/961 [==============================] - 1s 579us/step - loss: 0.0989 - accuracy: 0.4819"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.], dtype=float32)"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# base_model_data[0:32]\n",
    "base_model(base_model_data[0:1])\n",
    "# base_model_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0095, 0.5216202735900879)"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, accuracy = base_model.evaluate(base_model_data, base_model_labels,\n",
    "                                    #  batch_size = batch_size, \n",
    "                                     verbose=0)\n",
    "\n",
    "round(loss,4),accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# class MyModel(tf.keras.Model):\n",
    "\n",
    "#   def __init__(self):\n",
    "#     super().__init__()\n",
    "#     flatten = tf.keras.layers.Flatten()\n",
    "#     conv_layer1 = tf.keras.layers.Conv1D(filters = 64, kernel_size = 3)\n",
    "#     conv_layer2 = tf.keras.layers.Conv1D(filters = 64, kernel_size = 3)\n",
    "#     dropout = tf.keras.layers.Dropout(0.5)\n",
    "#     max_pooling = tf.keras.layers.MaxPool1D(pool_size=2)\n",
    "#     # fc - fully connected layer\n",
    "#     fc_layer = tf.keras.layers.Dense(units=64, activation = 'relu')\n",
    "#     fc_layer2 = tf.keras.layers.Dense(units=1, activation = 'softmax') \n",
    "#     self.base_model = tf.keras.Sequential([\n",
    "#                                   conv_layer1, \n",
    "#                                   conv_layer2, \n",
    "#                                   dropout, \n",
    "#                                   max_pooling, \n",
    "#                                   # flatten, \n",
    "#                                   fc_layer, \n",
    "#                                   fc_layer2\n",
    "#                                 ])\n",
    "\n",
    "\n",
    "#   def call(self, inputs, training=True):\n",
    "#     return self.base_model(inputs)\n",
    "    \n",
    "#     # if training:\n",
    "#     #   x = self.dropout(x, training=training)\n",
    "#     # return self.dense2(x)\n",
    "\n",
    "# model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), \\\n",
    "#                    optimizer=tf.keras.optimizers.Adam(0.001), \\\n",
    "#                    metrics=[tf.keras.metrics.Accuracy()]) \n",
    "\n",
    "# model.build((30735, 3, 200))\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(final_arr_reshape, labels_arr_reshape, epochs = 100,\n",
    "#             #    batch_size = batch_size, \n",
    "#                verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 50\n",
    "# prep_data = pd.read_csv('../data/good_again_bhas.csv')\n",
    "flatten = tf.keras.layers.Flatten()\n",
    "conv_layer1 = tf.keras.layers.Conv1D(filters = 64, kernel_size = 3, padding='SAME')\n",
    "conv_layer2 = tf.keras.layers.Conv1D(filters = 64, kernel_size = 3, padding='SAME')\n",
    "dropout = tf.keras.layers.Dropout(0.5)\n",
    "max_pooling = tf.keras.layers.MaxPool1D(pool_size=2)\n",
    "# fc - fully connected layer\n",
    "fc_layer = tf.keras.layers.Dense(units=128, activation = 'relu')\n",
    "fc_layer2 = tf.keras.layers.Dense(units=1, activation = 'softmax')\n",
    "base_model = tf.keras.Sequential([\n",
    "                                  conv_layer1,  \n",
    "                                  conv_layer2, \n",
    "                                  dropout, \n",
    "                                  max_pooling, \n",
    "                                  flatten, \n",
    "                                  fc_layer, \n",
    "                                  fc_layer2\n",
    "                                ])\n",
    "\n",
    "# base_model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False, label_smoothing=0.0001), \\\n",
    "base_model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), \\\n",
    "                   optimizer=tf.keras.optimizers.Adam(0.001), \\\n",
    "                   metrics=[tf.keras.metrics.Accuracy()]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_func = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "# optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "\n",
    "# for e in range(0, 5000):\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         preds = base_model(final_arr_reshape)\n",
    "#         print(preds)\n",
    "        \n",
    "#         # loss = loss_func(y_true=labels_arr_reshape, y_pred=preds)\n",
    "#         # acc = np.sum(np.equal(labels, preds)) / 30735\n",
    "#     #     print(acc)\n",
    "#     # gradients = tape.gradient(loss, base_model.trainable_variables)\n",
    "#     # optimizer.apply_gradients(zip(gradients, base_model.trainable_variables))\n",
    "#     # print(f\"Epoch: {e} | LOSS : {loss} | acc {acc}\")\n",
    "\n",
    "# # loss\n",
    "            \n",
    "# # total_loss += loss\n",
    "# # gradients = tape.gradient(loss, model.trainable_variables)\n",
    "# #     optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "# #     return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.build((30735, 200, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_57\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_48 (Conv1D)          (30735, 200, 64)          640       \n",
      "                                                                 \n",
      " conv1d_49 (Conv1D)          (30735, 200, 64)          12352     \n",
      "                                                                 \n",
      " dropout_31 (Dropout)        (30735, 200, 64)          0         \n",
      "                                                                 \n",
      " max_pooling1d_24 (MaxPoolin  (30735, 100, 64)         0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten_55 (Flatten)        (30735, 6400)             0         \n",
      "                                                                 \n",
      " dense_140 (Dense)           (30735, 128)              819328    \n",
      "                                                                 \n",
      " dense_141 (Dense)           (30735, 1)                129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 832,449\n",
      "Trainable params: 832,449\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 1.], dtype=float32), array([11238, 19497]))"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(labels_arr, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1225727, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tac\n",
       "1    778034\n",
       "0    447693\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(all_labels_df.shape)\n",
    "all_labels_df['tac'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "961/961 [==============================] - 7s 7ms/step - loss: 0.6206 - accuracy: 0.6344\n",
      "Epoch 2/50\n",
      "961/961 [==============================] - 7s 7ms/step - loss: 0.5941 - accuracy: 0.6344\n",
      "Epoch 3/50\n",
      "961/961 [==============================] - 7s 7ms/step - loss: 0.5795 - accuracy: 0.6344\n",
      "Epoch 4/50\n",
      "136/961 [===>..........................] - ETA: 5s - loss: 0.5739 - accuracy: 0.6321"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[394], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m base_model\u001b[39m.\u001b[39;49mfit(final_arr_reshape, labels_arr_reshape, epochs \u001b[39m=\u001b[39;49m epochs,\n\u001b[1;32m      2\u001b[0m                batch_size \u001b[39m=\u001b[39;49m batch_size, \n\u001b[1;32m      3\u001b[0m                verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m) \n\u001b[1;32m      5\u001b[0m \u001b[39m# base_model.fit(base_model_data, base_model_labels, epochs = 100,\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m#             #    batch_size = batch_size, \u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39m#                verbose=1) \u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.10/site-packages/keras/engine/training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1642\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1643\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1644\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1648\u001b[0m ):\n\u001b[1;32m   1649\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1650\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1651\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1652\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    909\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    910\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    911\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 912\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    915\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    916\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m   (concrete_function,\n\u001b[1;32m    133\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    135\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1741\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1742\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1743\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1744\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1746\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1747\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m     args,\n\u001b[1;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1750\u001b[0m     executing_eagerly)\n\u001b[1;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    377\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    379\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    380\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    381\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    382\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    383\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    384\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    386\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    387\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    391\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "base_model.fit(final_arr_reshape, labels_arr_reshape, epochs = epochs,\n",
    "               batch_size = batch_size, \n",
    "               verbose=1) \n",
    "\n",
    "# base_model.fit(base_model_data, base_model_labels, epochs = 100,\n",
    "#             #    batch_size = batch_size, \n",
    "#                verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5554, 0.6346510648727417)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss, accuracy = base_model.evaluate(final_arr_reshape, labels_arr_reshape,\n",
    "#                                     #  batch_size = batch_size, \n",
    "#                                      verbose=0)\n",
    "\n",
    "loss, accuracy = base_model.evaluate(base_model_data, base_model_labels,\n",
    "                                    #  batch_size = batch_size, \n",
    "                                     verbose=0)\n",
    "\n",
    "round(loss,4), accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "clf = MLPClassifier(solver='adam', shuffle=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarah_prakriti_peters/miniconda3/envs/DL/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1098: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/sarah_prakriti_peters/miniconda3/envs/DL/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'activation': 'relu',\n",
       " 'alpha': 0.0001,\n",
       " 'batch_size': 'auto',\n",
       " 'beta_1': 0.9,\n",
       " 'beta_2': 0.999,\n",
       " 'early_stopping': False,\n",
       " 'epsilon': 1e-08,\n",
       " 'hidden_layer_sizes': (100,),\n",
       " 'learning_rate': 'constant',\n",
       " 'learning_rate_init': 0.001,\n",
       " 'max_fun': 15000,\n",
       " 'max_iter': 200,\n",
       " 'momentum': 0.9,\n",
       " 'n_iter_no_change': 10,\n",
       " 'nesterovs_momentum': True,\n",
       " 'power_t': 0.5,\n",
       " 'random_state': 1,\n",
       " 'shuffle': True,\n",
       " 'solver': 'adam',\n",
       " 'tol': 0.0001,\n",
       " 'validation_fraction': 0.1,\n",
       " 'verbose': False,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_arr_mlp = np.reshape(final_arr, (30735, 600))\n",
    "labels_arr_mlp = np.reshape(labels_arr, (30735,1))\n",
    "\n",
    "clf.fit(final_arr_mlp, labels_arr_mlp)\n",
    "clf.get_params()\n",
    "\n",
    "# print('Accuracy ', accuracy_score(y_test, clf.predict(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy  0.9113714006832602\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy ', accuracy_score(labels_arr_mlp, clf.predict(final_arr_mlp)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model \n",
    "#### LSTM Layer (128 units), Dropout layer (p=0.5), Dense layer (128 units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_29\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_6 (LSTM)               (30735, 128)              67584     \n",
      "                                                                 \n",
      " dropout_23 (Dropout)        (30735, 128)              0         \n",
      "                                                                 \n",
      " dense_66 (Dense)            (30735, 128)              16512     \n",
      "                                                                 \n",
      " dense_67 (Dense)            (30735, 1)                129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 84,225\n",
      "Trainable params: 84,225\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model_data = np.reshape(final_arr, (30735, 200, 3)) \n",
    "base_model_labels = np.reshape(labels_arr, (30735,1))\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "# prep_data = pd.read_csv('../data/good_again_bhas.csv')\n",
    "flatten = tf.keras.layers.Flatten()\n",
    "lstm_layer = tf.keras.layers.LSTM(units=128)\n",
    "dropout_layer = tf.keras.layers.Dropout(0.5)\n",
    "fc_layer = tf.keras.layers.Dense(units=128, activation='relu', kernel_regularizer = 'l2')\n",
    "fc_layer2 = tf.keras.layers.Dense(units=1, activation='softmax')\n",
    "lstm_model = tf.keras.Sequential([lstm_layer, dropout_layer,\n",
    "                                # flatten, \n",
    "                                fc_layer, fc_layer2])\n",
    "\n",
    "lstm_model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), \\\n",
    "                   optimizer=tf.keras.optimizers.Adam(0.03), \\\n",
    "                   metrics=[tf.keras.metrics.Accuracy()]) \n",
    "\n",
    "lstm_model.build((30735, 200, 3))\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "961/961 [==============================] - 85s 86ms/step - loss: 0.6732 - accuracy: 0.6347\n",
      "Epoch 2/100\n",
      "961/961 [==============================] - 81s 85ms/step - loss: 0.6574 - accuracy: 0.6347\n",
      "Epoch 3/100\n",
      "961/961 [==============================] - 81s 84ms/step - loss: 0.6569 - accuracy: 0.6347\n",
      "Epoch 4/100\n",
      "961/961 [==============================] - 82s 85ms/step - loss: 0.6571 - accuracy: 0.6347\n",
      "Epoch 5/100\n",
      "961/961 [==============================] - 82s 85ms/step - loss: 0.6571 - accuracy: 0.6347\n",
      "Epoch 6/100\n",
      "961/961 [==============================] - 83s 86ms/step - loss: 0.6571 - accuracy: 0.6347\n",
      "Epoch 7/100\n",
      "266/961 [=======>......................] - ETA: 1:00 - loss: 0.6546 - accuracy: 0.6397"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[196], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m lstm_model\u001b[39m.\u001b[39;49mfit(base_model_data, base_model_labels, epochs \u001b[39m=\u001b[39;49m \u001b[39m100\u001b[39;49m,\n\u001b[1;32m      2\u001b[0m             \u001b[39m#    batch_size = batch_size, \u001b[39;49;00m\n\u001b[1;32m      3\u001b[0m                verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m) \n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.10/site-packages/keras/engine/training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1642\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1643\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1644\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1648\u001b[0m ):\n\u001b[1;32m   1649\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1650\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1651\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1652\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    909\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    910\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    911\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 912\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    915\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    916\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m   (concrete_function,\n\u001b[1;32m    133\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    135\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1741\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1742\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1743\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1744\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1746\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1747\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m     args,\n\u001b[1;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1750\u001b[0m     executing_eagerly)\n\u001b[1;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    377\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    379\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    380\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    381\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    382\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    383\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    384\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    386\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    387\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    391\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad pipe message: %s [b'\\xb9\\xe3\\xb2\\x16\\x96\\xe7\\xae\\xcd\\xd3\\x02\\x08M\\xd6\\xdc\\xa5u\\xeb\\xfd \\x11\\t\\xdc@ \\xa9>\\xb3\\xbb\\x88\\x18\\x94\\xca!B\\xab\\xbeaS\\xb6\\x88;\\x00\\xf5\\x15\\xc2\\x13A\\xd5M3@\\x00 ::\\x13\\x01\\x13\\x02\\x13\\x03\\xc0+\\xc0/\\xc0,\\xc00\\xcc\\xa9\\xcc\\xa8\\xc0\\x13\\xc0\\x14\\x00\\x9c\\x00\\x9d\\x00/\\x005\\x01\\x00\\x01\\x93\\xea\\xea\\x00\\x00\\x00\\x0b\\x00\\x02\\x01\\x00\\x00\\x17\\x00\\x00\\x00\\n\\x00\\n\\x00\\x08zz\\x00\\x1d\\x00\\x17\\x00\\x18\\x003\\x00+\\x00)zz\\x00\\x01\\x00\\x00\\x1d\\x00 lQ\\x8a\\x99\\x04\\xaf\\xc1\\x96\\x83\\x0f\\xa5,(\\xde1\\x18\\x9d\\xb1JEZ\"P\\xf5\\xba\\xf7\\xfdQy\\xe6{W\\x00\\x10\\x00\\x0e\\x00\\x0c\\x02h2\\x08http/1.1\\x00\\x12\\x00\\x00\\xff\\x01\\x00\\x01\\x00\\x00\\x05\\x00\\x05\\x01\\x00\\x00\\x00\\x00\\x00\\x1b\\x00\\x03\\x02\\x00\\x02\\x00-\\x00\\x02\\x01\\x01\\x00+\\x00\\x07\\x06jj\\x03\\x04\\x03\\x03\\x00#\\x00\\x00Di\\x00\\x05\\x00\\x03\\x02']\n",
      "Bad pipe message: %s [b'\\xfa}\\xe5\\xe9v\\r\\xc4a\\x8c\\xa0\\x9d\\x8d\\xb6a\\xb1$\\xbcG \\x12\\x9c2\\xab\\xb0\\xe0\\xbd3\\x00\\x8d\\xab\\xc5\\x86\\x81b.<\\xbb\\xaa\\xb6\\xd7\\xb6\\xc7\\x16+\\xb6f\\n\\x9d\\x91a\\xe4\\x00 zz\\x13\\x01\\x13\\x02\\x13\\x03\\xc0+\\xc0/\\xc0,\\xc00\\xcc\\xa9\\xcc\\xa8\\xc0\\x13\\xc0\\x14\\x00\\x9c\\x00\\x9d\\x00/\\x005\\x01\\x00\\x01\\x93\\xaa\\xaa\\x00\\x00\\x00\\x05\\x00\\x05\\x01\\x00\\x00\\x00\\x00\\x00#\\x00\\x00\\x00\\n\\x00\\n\\x00\\x08\\n\\n\\x00\\x1d\\x00\\x17\\x00', b'\\x01\\x00\\x01\\x00\\x003\\x00+\\x00)\\n\\n\\x00\\x01\\x00\\x00\\x1d\\x00 \\xb6\\xd4\\xbf\\xb5', b'\\xc1\\xe2Y\\xd6\\xfem\\xeam|f\\x93\\xe1:R\\x92\\xac\\x1a\\xc9\\x9b}jqw\\xf5\\xc2P\\x00+\\x00\\x07\\x06JJ\\x03\\x04\\x03\\x03\\x00-\\x00\\x02\\x01\\x01\\x00\\x0b\\x00\\x02\\x01\\x00\\x00\\x10\\x00\\x0e\\x00\\x0c\\x02h2\\x08http/1.1\\x00\\r\\x00\\x14\\x00\\x12\\x04\\x03\\x08\\x04\\x04\\x01\\x05\\x03\\x08\\x05\\x05\\x01\\x08\\x06\\x06\\x01\\x02\\x01\\x00\\x17\\x00\\x00\\x00\\x12\\x00\\x00Di\\x00\\x05\\x00\\x03\\x02h2\\x00\\x1b\\x00\\x03\\x02\\x00\\x02**\\x00\\x01\\x00\\x00\\x15\\x00\\xde']\n",
      "Bad pipe message: %s [b'\\x00\\r\\x00\\x12\\x00\\x10\\x04\\x03\\x08\\x04\\x04\\x01\\x05\\x03\\x08\\x05\\x05\\x01\\x08\\x06\\x06\\x01zz\\x00\\x01\\x00\\x00\\x15\\x00\\xe0\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00']\n"
     ]
    }
   ],
   "source": [
    "lstm_model.fit(base_model_data, base_model_labels, epochs = 100,\n",
    "            #    batch_size = batch_size, \n",
    "               verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "961/961 [==============================] - 2s 1ms/step - loss: 0.5258 - accuracy: 0.6347\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5258, 0.6346510648727417)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad pipe message: %s [b'\\x02P\\xcep\\xd4\\x9d\\xebA\\x8a7+\\x9a+\\xc0\\xbe\\x08\\xa8\\x9d 2\\xd4\\xb9\\xf5;\\xc4\\xe0\\xde\\xdeeKk\\x1e\\x01\\x8e\\xc3\\x86\\xb5\\x03\\x0e\\x8e\\x83\\x82@\\xfe\\x06\\x863Z\\x91V\\x87\\x00 **\\x13\\x01\\x13\\x02\\x13\\x03\\xc0+\\xc0/\\xc0,\\xc00\\xcc\\xa9\\xcc\\xa8\\xc0\\x13\\xc0\\x14\\x00\\x9c\\x00\\x9d\\x00/\\x005\\x01\\x00\\x01\\x93\\xba\\xba\\x00\\x00\\x00\\x05\\x00\\x05\\x01\\x00\\x00\\x00\\x00\\x00\\x1b\\x00\\x03\\x02\\x00\\x02\\x00\\x17\\x00\\x00\\x00\\n\\x00\\n\\x00\\x08\\x8a\\x8a\\x00\\x1d\\x00\\x17\\x00\\x18\\xff\\x01\\x00\\x01\\x00Di\\x00\\x05\\x00\\x03\\x02h2\\x00\\r\\x00\\x12\\x00\\x10\\x04\\x03\\x08\\x04\\x04\\x01\\x05\\x03\\x08\\x05\\x05\\x01\\x08\\x06\\x06\\x01\\x003\\x00+\\x00)\\x8a\\x8a\\x00\\x01\\x00\\x00\\x1d\\x00 \\x15\\xbb\\xcc+\\xdf\\xd6R\\xc2\\xf4i\\x96\\xd2{x^\\xe4#\\xd4>\\x9cq\\xb6[']\n",
      "Bad pipe message: %s [b'\\x8e\\x16\\xea\\xe5\\x02\\x89\\x08\\x00+\\x00\\x07\\x06\\xba\\xba\\x03\\x04\\x03\\x03\\x00\\x0b\\x00\\x02\\x01\\x00\\x00\\x10\\x00\\x0e\\x00\\x0c\\x02h2\\x08http/1.1\\x00-\\x00\\x02\\x01\\x01\\x00\\x12\\x00\\x00\\x00#\\x00\\x00\\xfa\\xfa\\x00\\x01\\x00\\x00\\x15\\x00\\xe0\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00']\n",
      "Bad pipe message: %s [b'R\\xdd\\xc8\\xca~u\\xa6>`\\x0fD;\\xfe\\x81\\x92\\x1ag( \\xf3\\xed\\xdf\\xda4\\x8d\\x08\\x17nE\\xbf\\x82w\\x9e\\x9b?z2\\x9d\\xe3\\x96\\xcc\\x1a(!\\xe3j\\x11U\\x95\\xd7\\x9b\\x00 jj\\x13\\x01\\x13\\x02\\x13\\x03\\xc0+\\xc0/\\xc0,\\xc00\\xcc\\xa9\\xcc\\xa8\\xc0\\x13\\xc0\\x14\\x00\\x9c\\x00\\x9d\\x00/\\x005\\x01\\x00\\x01\\x93\\xaa\\xaa\\x00\\x00\\xff\\x01\\x00\\x01\\x00\\x00\\x05\\x00\\x05\\x01\\x00\\x00\\x00\\x00Di\\x00\\x05\\x00\\x03\\x02h2\\x00-\\x00\\x02\\x01\\x01\\x00+\\x00\\x07\\x06\\xca\\xca\\x03\\x04\\x03\\x03\\x003\\x00+\\x00)jj\\x00\\x01\\x00\\x00\\x1d\\x00 \\xbf\\xec\\x06T\\x12\\xfa\\xd5\\x7f8z\\xdb\\x08D\"\\xfe\\xeaX\\xb6G\\x1e\\x1c\\xc5.\\xb8\\xee\\xe7m<&ova\\x00\\n\\x00\\n\\x00\\x08jj\\x00']\n",
      "Bad pipe message: %s [b'\\x17\\x00\\x18\\x00\\x10\\x00\\x0e\\x00\\x0c\\x02h2\\x08http/1.1\\x00\\x1b\\x00\\x03\\x02\\x00\\x02']\n",
      "Bad pipe message: %s [b'\\xcfdP<\\xe2\\xa0m\\x80\\xe7\\xe7~m\\xf6\\xdaI\\xe9\\xde\\xda \\x07I\\xaf\\x95\\x05\\xf8\\x1f\\xa3%', b'b2\\x9a\\x9ei\\xb0a\\x034\\xf4p\\x99\\xefmS\\\\\\x1eh\\x1e', b'\\x00 \\x1a\\x1a\\x13\\x01\\x13\\x02\\x13\\x03\\xc0+\\xc0/\\xc0,\\xc00\\xcc\\xa9\\xcc\\xa8\\xc0\\x13\\xc0\\x14\\x00\\x9c\\x00\\x9d\\x00/\\x005\\x01\\x00\\x01\\x93\\x8a\\x8a\\x00\\x00\\x00\\x0b\\x00\\x02\\x01\\x00\\x00\\x05\\x00\\x05\\x01\\x00\\x00\\x00\\x00\\x003\\x00+\\x00)\\xba\\xba\\x00\\x01\\x00\\x00\\x1d\\x00 \\x9dP\\xc3U\\tm\\xa0\\xc2\\x9b\\x9aS\\x01W6\\xbb\\x042\\xbb\\x17|\\xb4b\\x8d\\xefqh\\x14l<\\x15\\xb9\\x00\\x00\\x12\\x00\\x00\\x00\\n\\x00\\n\\x00\\x08\\xba\\xba\\x00\\x1d\\x00\\x17\\x00\\x18\\x00\\x1b\\x00\\x03\\x02\\x00\\x02\\x00-\\x00\\x02\\x01\\x01\\x00#\\x00\\x00']\n",
      "Bad pipe message: %s [b'r\\xbbx\\xae^U\\xf2o\\xee;\\xd7\\xcb\\xa5\\xce\\xcd\\x01\\x85\\xdf :\\x84<\\x0c\\xe0!//)\\x18\\xc0m\\xd9\\x14~\\xc3\\x14\\xe5\\xb0\\xd4[R\\xb8\\x17\\xf5\\xdb\\x88\\x98\\x88+;\\x17\\x00 \\x8a\\x8a\\x13\\x01\\x13\\x02\\x13\\x03\\xc0+\\xc0/\\xc0,\\xc00\\xcc\\xa9\\xcc\\xa8\\xc0\\x13\\xc0\\x14\\x00\\x9c\\x00\\x9d\\x00/\\x005\\x01\\x00\\x01\\x93zz\\x00\\x00\\x00#\\x00\\x00\\x00\\n\\x00\\n\\x00\\x08**\\x00\\x1d\\x00\\x17\\x00\\x18\\x00\\x05\\x00\\x05\\x01\\x00\\x00\\x00\\x00\\xff\\x01\\x00\\x01\\x00\\x00+\\x00\\x07\\x06\\xaa\\xaa\\x03\\x04\\x03\\x03']\n",
      "Bad pipe message: %s [b\"\\xf8N\\xac\\xf8\\xe9}\\x86'\\x9fd\\xb4\\xa6<\\xee\\xa7!\\xc2\\xaa \\xad\\xd1\\tV@\\xddu\\xd5R\\xb1\\x9e\\x19\\x19\\xac,\\xf6\\xca\\x16\\x8eOp\\xc7\\x83/\\xef\\x02\\x0c\\x17\\xcf0\\x1f\\xb9\\x00 \\xaa\\xaa\\x13\\x01\\x13\\x02\\x13\\x03\\xc0+\\xc0/\\xc0,\\xc00\\xcc\\xa9\\xcc\\xa8\\xc0\\x13\\xc0\\x14\\x00\\x9c\\x00\\x9d\\x00/\\x005\\x01\\x00\\x01\\x93ZZ\\x00\\x00\\x00\\x1b\\x00\\x03\\x02\\x00\\x02\\x00\\x12\\x00\\x00\\x00#\\x00\\x00\\x00\\x0b\\x00\\x02\\x01\\x00\\x00\\x10\\x00\\x0e\\x00\\x0c\\x02h2\\x08http\", b'.1\\x003\\x00+\\x00)JJ\\x00\\x01\\x00\\x00\\x1d\\x00 \\xc5\\x91\\xf5#Ls\\x8a\\xaasRW5\\xeeN\\xf6\\xadY35f\\xf2\\xce*\\x18\\xac2A\\x83\\x00', b'q\\x00\\n']\n",
      "Bad pipe message: %s [b\"o\\x8b'XF\\x82\\x02\\xae\\xc3\\x17\\xea\\xa6\\xb6\\x84\"]\n",
      "Bad pipe message: %s [b\"-\\x1e \\xbf\\x84\\xf1a\\xd0\\xd6\\xee\\x81#q\\x81*\\x89u\\xea\\xc5Pt\\x07I\\xd6\\x01\\xb0n\\xf3o'\\xf0h\\x18\\xfb\\x11\\x00 \\x1a\\x1a\\x13\\x01\\x13\\x02\\x13\\x03\\xc0+\\xc0/\\xc0,\\xc00\\xcc\\xa9\\xcc\\xa8\\xc0\\x13\\xc0\\x14\\x00\\x9c\\x00\\x9d\\x00/\\x005\\x01\\x00\\x01\\x93\\xba\\xba\\x00\\x00\\x00\\r\\x00\\x14\\x00\\x12\\x04\\x03\\x08\\x04\\x04\\x01\\x05\\x03\\x08\\x05\\x05\\x01\\x08\\x06\\x06\\x01\\x02\\x01\\x00\\n\\x00\\n\\x00\\x08\\xda\\xda\\x00\\x1d\\x00\\x17\\x00\\x18Di\\x00\", b'\\x03\\x02h2']\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = lstm_model.evaluate(base_model_data, base_model_labels,\n",
    "                                    #  batch_size = batch_size, \n",
    "                                     verbose=1)\n",
    "\n",
    "round(loss,4),accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('csci1470')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d9ae1d7aab292b742cce88b46a7d39d3e8f6dede2c2111bf10ed84b03dba6379"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
